{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Iterable, Iterator, Optional, Protocol, Union\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom functools import wraps\nimport warnings\nIS_CI = os.getenv(\"IS_CI\")\nArr = np.ndarray\ngrad_tracking_enabled = True\n\nimport torch\nimport torch.utils.data\nfrom torchvision import datasets, transforms\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:37:11.385615Z","iopub.execute_input":"2023-09-09T21:37:11.386012Z","iopub.status.idle":"2023-09-09T21:37:15.025602Z","shell.execute_reply.started":"2023-09-09T21:37:11.385983Z","shell.execute_reply":"2023-09-09T21:37:15.024363Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"TEST_FN_PASSED = {}\n\n\ndef run_and_report(test_func: Callable, name: str, *test_func_args, **test_func_kwargs):\n    start = time.time()\n    out = test_func(*test_func_args, **test_func_kwargs)\n    elapsed = time.time() - start\n    print(f\"{name} passed in {elapsed:.2f}s.\")\n    if not TEST_FN_PASSED.get(name):\n        report_success(name)\n        TEST_FN_PASSED[name] = True\n    return out\n\ndef report(test_func):\n    name = f\"{test_func.__module__}.{test_func.__name__}\"\n    # This can happen when using autoreload, so don't complain about it.\n    # if name in TEST_FN_PASSED:\n    #     raise KeyError(f\"Already registered: {name}\")\n    TEST_FN_PASSED[name] = False\n\n    @wraps(test_func)\n    def wrapper(*args, **kwargs):\n        return run_and_report(test_func, name, *args, **kwargs)\n\n    return wrapper\n\n@report\ndef test_log_back(log_back):\n    a = np.array([1, np.e, np.e**np.e])\n    b = np.log(a)\n    grad_out = np.array([2.0, 2.0, 2.0])\n    actual = log_back(grad_out, b, a)\n    expected = [2.0, 2.0 / np.e, 2.0 / (np.e**np.e)]\n    assert np.allclose(actual, expected)\n    \n\ndef report_success(testname):\n    \"\"\"POST to the server indicating success at the given test.\n\n    Used to help the TAs know how long each section takes to complete.\n    \"\"\"\n    server = os.environ.get(\"MLAB_SERVER\")\n    email = os.environ.get(\"MLAB_EMAIL\")\n    if server:\n        if email:\n            r = requests.post(\n                server + \"/api/report_success\",\n                json=dict(email=email, testname=testname),\n            )\n            if r.status_code != http.HTTPStatus.NO_CONTENT:\n                raise ValueError(f\"Got status code from server: {r.status_code}\")\n        else:\n            raise ValueError(f\"Server set to {server} but no MLAB_EMAIL set!\")\n    else:\n        if email:\n            raise ValueError(f\"Email set to {email} but no MLAB_SERVER set!\")\n        else:\n            return  # local dev, do nothing\n        \n@report\ndef test_unbroadcast(unbroadcast):\n    small = np.ones((2, 1, 3))\n    large = np.broadcast_to(small, (5, 1, 2, 4, 3))\n    out = unbroadcast(large, small)\n    assert out.shape == small.shape\n    assert (out == 20.0).all(), \"Each element in the small array appeared 20 times in the large array.\"\n\n    small = np.ones((2, 1, 3))\n    large = np.broadcast_to(small, (5, 1, 2, 1, 3))\n    out = unbroadcast(large, small)\n    assert out.shape == small.shape\n    assert (out == 5.0).all(), \"Each element in the small array appeared 5 times in the large array.\"\n\n    small = np.ones((2, 1, 3))\n    large = np.broadcast_to(small, (2, 4, 3))\n    out = unbroadcast(large, small)\n    assert out.shape == small.shape\n    assert (out == 4.0).all(), \"Each element in the small array appeared 4 times in the large array.\"\n    \n@report\ndef test_multiply_back(multiply_back0, multiply_back1):\n    a = np.array([1, 2, 3])\n    b = np.array([2])\n    c = a * b\n    grad_out = np.array([2.0, 2.0, 2.0])\n    actual = multiply_back0(grad_out, c, a, b)\n    expected = [4.0, 4.0, 4.0]\n    assert np.allclose(actual, expected)\n    actual = multiply_back1(grad_out, c, a, b)\n    expected = [12.0]\n    assert np.allclose(actual, expected)\n\n\n@report\ndef test_multiply_back_float(multiply_back0, multiply_back1):\n    a = np.array([1, 2, 3])\n    b = 2\n    c = a * b\n    grad_out = np.array([2.0, 2.0, 2.0])\n    actual = multiply_back0(grad_out, c, a, b)\n    expected = [4.0, 4.0, 4.0]\n    assert np.allclose(actual, expected)\n\n    a = np.array([1, 2, 3])\n    b = 2\n    c = a * b\n    grad_out = np.array([2.0, 2.0, 2.0])\n    actual = multiply_back1(grad_out, c, b, a)\n    expected = [4.0, 4.0, 4.0]\n    assert np.allclose(actual, expected)\n    \n@report\ndef test_forward_and_back(forward_and_back):\n    a = np.array([1, 2, 3])\n    b = np.array([2, 3, 1])\n    c = np.array([10])\n    dg_da, dg_db, dg_dc = forward_and_back(a, b, c)\n    expected_dg_da = np.array([1, 1 / 2, 1 / 3])\n    expected_dg_db = np.array([1 / 2, 1 / 3, 1])\n    expected_dg_dc = np.array([0.13028834])\n    assert np.allclose(dg_da, expected_dg_da)\n    assert np.allclose(dg_db, expected_dg_db)\n    assert np.allclose(dg_dc, expected_dg_dc)\n    \n@report\ndef test_back_func_lookup(BackwardFuncLookup):\n    backward_funcs = BackwardFuncLookup()\n    backward_funcs.add_back_func(np.log, 0, np.exp)\n    assert backward_funcs.get_back_func(np.log, 0) == np.exp\n    backward_funcs.add_back_func(np.multiply, 0, np.divide)\n    assert backward_funcs.get_back_func(np.multiply, 0) == np.divide\n    backward_funcs.add_back_func(np.multiply, 1, np.add)\n    assert backward_funcs.get_back_func(np.multiply, 1) == np.add\n    \n\n\n@report\ndef test_log(Tensor, log_forward):\n    a = Tensor([np.e, np.e**np.e], requires_grad=True)\n    b = log_forward(a)\n    assert np.allclose(b.array, [1, np.e])\n    assert b.requires_grad == True, \"Should require grad because input required grad.\"\n    assert b.is_leaf == False\n    assert b.recipe is not None\n    assert len(b.recipe.parents) == 1 and b.recipe.parents[0] is a\n    assert len(b.recipe.args) == 1 and b.recipe.args[0] is a.array\n    assert b.recipe.kwargs == {}\n    assert b.recipe.func is np.log\n    c = log_forward(b)\n    assert np.allclose(c.array, [0, 1])\n\n\n@report\ndef test_log_no_grad(Tensor, log_forward):\n    d = Tensor([1, np.e])\n    e = log_forward(d)\n    assert e.requires_grad == False, \"Should not require grad because input did not.\"\n    assert e.recipe is None\n    assert np.allclose(e.array, [0, 1])\n    \n\n@report\ndef test_multiply(Tensor, multiply):\n    a = Tensor([0, 1, 2, 3], requires_grad=True)\n    b = Tensor([[0], [1], [10]], requires_grad=True)\n    c = multiply(a, b)\n    assert c.requires_grad == True, \"Should require grad because input required grad.\"\n    assert c.is_leaf == False\n    assert c.recipe is not None\n    assert len(c.recipe.parents) == 2 and c.recipe.parents[0] is a and c.recipe.parents[1] is b\n    assert len(c.recipe.args) == 2 and c.recipe.args[0] is a.array and c.recipe.args[1] is b.array\n    assert c.recipe.kwargs == {}\n    assert c.recipe.func is np.multiply\n    expected = np.array([[0, 0, 0, 0], [0, 1, 2, 3], [0, 10, 20, 30]])\n    np.allclose(c.array, expected)\n\n\n@report\ndef test_multiply_no_grad(Tensor, multiply):\n    a = Tensor([0, 1, 2, 3], requires_grad=False)\n    b = Tensor([[0], [1], [10]], requires_grad=False)\n    c = multiply(a, b)\n    assert c.requires_grad == False, \"Should not require grad because input did not require grad.\"\n    assert c.recipe is None\n    expected = np.array([[0, 0, 0, 0], [0, 1, 2, 3], [0, 10, 20, 30]])\n    np.allclose(c.array, expected)\n\n\n@report\ndef test_multiply_float(Tensor, multiply):\n    a = Tensor([0, 1, 2, 3], requires_grad=True)\n    b = 3\n    c = multiply(a, b)\n    assert c.requires_grad == True\n    assert c.recipe is not None\n    assert len(c.recipe.parents) == 1 and c.recipe.parents[0] is a\n    assert len(c.recipe.args) == 2 and c.recipe.args[0] is a.array and c.recipe.args[1] is b\n    assert c.recipe.kwargs == {}\n    assert c.recipe.func is np.multiply\n    expected = np.array([0, 3, 6, 9])\n    np.allclose(c.array, expected)\n\n    a = Tensor([0, 1, 2, 3], requires_grad=True)\n    b = 3\n    c = multiply(b, a)\n    assert c.requires_grad == True\n    assert c.recipe is not None\n    assert len(c.recipe.parents) == 1 and c.recipe.parents[1] is a\n    assert len(c.recipe.args) == 2 and c.recipe.args[0] is b and c.recipe.args[1] is a.array\n    assert c.recipe.kwargs == {}\n    assert c.recipe.func is np.multiply\n    expected = np.array([0, 3, 6, 9])\n    np.allclose(c.array, expected)\n    \n@report\ndef test_topological_sort_linked_list(topological_sort):\n    z = Node()\n    y = Node(z)\n    x = Node(y)\n\n    expected = [z, y, x]\n    for e, a in zip(expected, topological_sort(x, get_children)):\n        assert e is a\n        \n        \n@report\ndef test_topological_sort_branching(topological_sort):\n    z = Node()\n    y = Node()\n    x = Node(y, z)\n    w = Node(x)\n\n    name_lookup = {w: \"w\", x: \"x\", y: \"y\", z: \"z\"}\n    out = \"\".join([name_lookup[n] for n in topological_sort(w, get_children)])\n    assert out == \"zyxw\" or out == \"yzxw\"\n    \n@report\ndef test_topological_sort_rejoining(topological_sort):\n    z = Node()\n    y = Node(z)\n    x = Node(y)\n    w = Node(z, x)\n\n    name_lookup = {w: \"w\", x: \"x\", y: \"y\", z: \"z\"}\n    out = \"\".join([name_lookup[n] for n in topological_sort(w, get_children)])\n    assert out == \"zyxw\"\n    \n\n@report\ndef test_topological_sort_cyclic(topological_sort):\n    z = Node()\n    y = Node(z)\n    x = Node(y)\n    z.children = [x]\n\n    try:\n        topological_sort(x, get_children)\n    except:\n        assert True\n    else:\n        assert False\n        \n\n@report\ndef test_backprop(Tensor):\n    a = Tensor([np.e, np.e**np.e], requires_grad=True)\n    b = a.log()\n    c = b.log()\n    c.backward(end_grad=np.array([1.0, 1.0]))\n    assert c.grad is None\n    assert b.grad is None\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, 1 / b.array / a.array)\n\n\n@report\ndef test_backprop_branching(Tensor):\n    a = Tensor([1, 2, 3], requires_grad=True)\n    b = Tensor([1, 2, 3], requires_grad=True)\n    c = a * b\n    c.backward(end_grad=np.array([1.0, 1.0, 1.0]))\n    assert np.allclose(a.grad.array, b.array)\n    assert np.allclose(b.grad.array, a.array)\n\n\n@report\ndef test_backprop_requires_grad_false(Tensor):\n    a = Tensor([1, 2, 3], requires_grad=True)\n    b = Tensor([1, 2, 3], requires_grad=False)\n    c = a * b\n    c.backward(end_grad=np.array([1.0, 1.0, 1.0]))\n    assert np.allclose(a.grad.array, b.array)\n    assert b.grad is None\n\n\n@report\ndef test_backprop_float_arg(Tensor):\n    a = Tensor([1, 2, 3], requires_grad=True)\n    b = 2\n    c = a * b\n    d = 2\n    e = d * c\n    e.backward(end_grad=np.array([1.0, 1.0, 1.0]))\n    assert e.grad is None\n    assert c.grad is None\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, np.array([4.0, 4.0, 4.0]))\n    \n@report\ndef test_negative_back(Tensor):\n    a = Tensor([-1, 0, 1], requires_grad=True)\n    b = -a\n    c = -b\n    c.backward(end_grad=np.array([[1.0, 1.0, 1.0]]))\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, [1, 1, 1])\n    \n\n@report\ndef test_exp_back(Tensor):\n    a = Tensor([-1.0, 0.0, 1.0], requires_grad=True)\n    b = a.exp()\n    b.backward(end_grad=np.array([[1.0, 1.0, 1.0]]))\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, 1 / np.e, 0, np.e)\n\n    a = Tensor([-1.0, 0.0, 1.0], requires_grad=True)\n    b = a.exp()\n    c = b.exp()\n    c.backward(end_grad=np.array([[1.0, 1.0, 1.0]]))\n\n    def d(x):\n        return (np.e**x) * (np.e ** (np.e**x))\n\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, *[d(x) for x in a.array])\n    \n@report\ndef test_reshape_back(Tensor):\n    a = Tensor([1, 2, 3, 4, 5, 6], requires_grad=True)\n    b = a.reshape((3, 2))\n    b.backward(end_grad=np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]))\n    assert a.grad is not None and np.allclose(a.grad.array, np.ones(6))\n    \n@report\ndef test_permute_back(Tensor):\n    a = Tensor(np.arange(24).reshape((2, 3, 4)), requires_grad=True)\n    out = a.permute((2, 0, 1))\n    out.backward(np.arange(24).reshape((4, 2, 3)))\n\n    assert a.grad is not None\n    assert np.allclose(\n        a.grad.array,\n        np.array(\n            [\n                [[0.0, 6.0, 12.0, 18.0], [1.0, 7.0, 13.0, 19.0], [2.0, 8.0, 14.0, 20.0]],\n                [[3.0, 9.0, 15.0, 21.0], [4.0, 10.0, 16.0, 22.0], [5.0, 11.0, 17.0, 23.0]],\n            ]\n        ),\n    )\n    \n@report\ndef test_expand(Tensor):\n    a = Tensor(np.ones((2, 1, 3)), requires_grad=True)\n    b = a.expand((5, 1, 2, 4, 3))\n    b.backward(np.full_like(b.array, 10.0))\n    assert a.grad is not None and a.grad.shape == a.array.shape\n    assert (a.grad.array == 20 * 10.0).all()\n\n\n@report\ndef test_expand_negative_length(Tensor):\n    a = Tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True)\n    b = a.expand((3, 2, -1))\n    assert b.shape == (3, 2, 5)\n    b.backward(end_grad=np.ones(b.shape))\n    assert a.grad is not None and a.grad.shape == a.array.shape\n    assert (a.grad.array == 6).all()\n    \n@report\ndef test_sum_keepdim_false(Tensor):\n    a = Tensor(np.array([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]]), requires_grad=True)\n    b = a.sum(0)\n    c = b.sum(0)\n    c.backward(np.array(2))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == 2).all()\n\n\n@report\ndef test_sum_keepdim_true(Tensor):\n    a = Tensor(np.array([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]]), requires_grad=True)\n    b = a.sum(1, keepdim=True)\n    c = a.sum(0, keepdim=True)\n    assert np.allclose(c.array, np.array([[5.0, 7.0, 9.0, 11.0, 13.0]]))\n    c.backward(end_grad=np.ones(c.shape))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == 1).all()\n\n\n@report\ndef test_sum_dim_none(Tensor):\n    a = Tensor(np.array([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]]), requires_grad=True)\n    b = a.sum()\n    b.backward(np.array(4))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == 4).all()\n    \n\n@report\ndef test_getitem_int(Tensor):\n    a = Tensor([[0, 1, 2], [3, 4, 5]], requires_grad=True)\n    b = a[1]\n    c = b.sum(0)\n    c.backward(np.array(10.0))\n    assert a.grad is not None and np.allclose(a.grad.array, np.array([[0, 0, 0], [10, 10, 10]]))\n\n\n@report\ndef test_getitem_tuple(Tensor):\n    a = Tensor([[0, 1, 2], [3, 4, 5]], requires_grad=True)\n    b = a[(1, 2)]\n    b.backward(np.array(10.0))\n    assert a.grad is not None and np.allclose(a.grad.array, np.array([[0, 0, 0], [0, 0, 10]]))\n\n\n@report\ndef test_getitem_integer_array(Tensor):\n    a = Tensor([[0, 1, 2], [3, 4, 5]], requires_grad=True)\n    index = np.array([0, 1, 0, 1, 0]), np.array([0, 0, 1, 2, 0])\n    out = a[index]\n    out.sum().backward(np.array(10.0))\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, np.array([[20, 10, 0], [10, 0, 10]]))\n\n\n@report\ndef test_getitem_integer_tensor(Tensor):\n    a = Tensor([[0, 1, 2], [3, 4, 5]], requires_grad=True)\n    index = Tensor(np.array([0, 1, 0, 1, 0])), Tensor(np.array([0, 0, 1, 2, 0]))\n    out = a[index]\n    out.sum().backward(np.array(10.0))\n    assert a.grad is not None\n    assert np.allclose(a.grad.array, np.array([[20, 10, 0], [10, 0, 10]]))\n    \n\n@report\ndef test_add_broadcasted(Tensor):\n    a = Tensor([0, 1, 2, 3], requires_grad=True)\n    b = Tensor([[0], [1], [10]], requires_grad=True)\n    c = a + b\n    c.backward(end_grad=np.ones(c.shape))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == 3).all()\n    assert b.grad is not None\n    assert b.grad.shape == b.shape\n    assert (b.grad.array == 4).all()\n\n\n@report\ndef test_subtract_broadcasted(Tensor):\n    a = Tensor([0, 1, 2, 3], requires_grad=True)\n    b = Tensor([[0], [1], [10]], requires_grad=True)\n    c = a - b\n    c.backward(end_grad=np.ones(c.shape))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == 3).all()\n    assert b.grad is not None\n    assert b.grad.shape == b.shape\n    assert (b.grad.array == -4).all()\n\n\n@report\ndef test_truedivide_broadcasted(Tensor):\n    a = Tensor([0, 6, 12, 18], requires_grad=True)\n    b = Tensor([[1], [2], [3]], requires_grad=True)\n    c = a / b\n    c.backward(end_grad=np.ones(c.shape))\n    assert a.grad is not None\n    assert a.grad.shape == a.shape\n    assert (a.grad.array == (1 + 1 / 2 + 1 / 3)).all()\n    assert b.grad is not None\n    assert b.grad.shape == b.shape\n    assert np.equal(b.grad.array, np.array([[-36.0], [-9.0], [-4.0]])).all()\n    \n@report\ndef test_maximum(Tensor):\n    a = Tensor([0, 1, 2], requires_grad=True)\n    b = Tensor([-1, 1, 3], requires_grad=True)\n    out = a.maximum(b)\n    assert np.allclose(out.array, [0, 1, 3])\n    out.backward(end_grad=np.ones(out.shape))\n\n    assert a.grad is not None\n    assert b.grad is not None\n    assert np.allclose(a.grad.array, [1, 0.5, 0])\n    assert np.allclose(b.grad.array, [0, 0.5, 1])\n\n\n@report\ndef test_maximum_broadcasted(Tensor):\n    a = Tensor([0, 1, 2], requires_grad=True)\n    b = Tensor([[-1], [1], [3]], requires_grad=True)\n    out = a.maximum(b)\n    assert np.allclose(out.array, np.array([[0, 1, 2], [1, 1, 2], [3, 3, 3]]))\n    out.backward(end_grad=np.ones(out.shape))\n    assert a.grad is not None and np.allclose(a.grad.array, np.array([1.0, 1.5, 2.0]))\n    assert b.grad is not None and np.allclose(b.grad.array, np.array([[0.0], [1.5], [3.0]]))\n    \n\n@report\ndef test_relu(Tensor):\n    a = Tensor([-1, 0, 1], requires_grad=True)\n    out = a.relu()\n    out.backward(end_grad=np.ones(out.shape))\n    assert a.grad is not None and np.allclose(a.grad.array, np.array([0, 0.5, 1.0]))\n    \n\n@report\ndef test_matmul2d(Tensor):\n    a = Tensor(np.arange(-3, 3).reshape((2, 3)), requires_grad=True)\n    b = Tensor(np.arange(-4, 5).reshape((3, 3)), requires_grad=True)\n    out = a @ b\n    out.backward(end_grad=np.ones(out.shape))\n    assert a.grad is not None\n    assert b.grad is not None\n    assert np.allclose(a.grad.array, np.array([[-9, 0, 9], [-9, 0, 9]]))\n    assert np.allclose(b.grad.array, np.array([[-3, -3, -3], [-1, -1, -1], [1, 1, 1]]))\n    \n@report\ndef test_cross_entropy(Tensor, cross_entropy):\n    logits = Tensor([[float(\"-inf\"), float(\"-inf\"), 0], [1 / 3, 1 / 3, 1 / 3], [float(\"-inf\"), 0, 0]])\n    true_labels = Tensor([2, 0, 0])\n    expected = Tensor([0.0, np.log(3), float(\"inf\")])\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", RuntimeWarning)\n        actual = cross_entropy(logits, true_labels)\n    assert np.allclose(actual.array, expected.array)\n    \n\n@report\ndef test_no_grad(Tensor, NoGrad):\n    a = Tensor([1], requires_grad=True)\n    with NoGrad():\n        b = a + a\n    c = a + a\n    assert b.requires_grad == False\n    assert b.recipe is None\n    assert c.requires_grad\n    assert c.recipe is not None\n\n\n@report\ndef test_no_grad_nested(Tensor, NoGrad):\n    a = Tensor([1], requires_grad=True)\n    with NoGrad():\n        with NoGrad():\n            with NoGrad():\n                b = a + a\n    assert b.requires_grad == False\n    assert b.recipe is None","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:33:31.029218Z","iopub.execute_input":"2023-09-09T21:33:31.029737Z","iopub.status.idle":"2023-09-09T21:33:31.170535Z","shell.execute_reply.started":"2023-09-09T21:33:31.029692Z","shell.execute_reply":"2023-09-09T21:33:31.169207Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def log_back(grad_out : Arr, Out : Arr, x : Arr) -> Arr:\n    \n    \"\"\"Backwards function for f(x) = log(x)\n    \n    grad_out: Gradient of some loss wrt out\n    out: the output of np.log(x). Provided as an optimization in case it's cheaper to express the gradient in terms of the output.\n    x: the input of np.log.\n    \n    Return: gradient of the given loss wrt x\n    \"\"\"\n    return (1 / x) * grad_out\n    \ntest_log_back(log_back)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:40.269716Z","iopub.execute_input":"2023-09-09T20:39:40.270055Z","iopub.status.idle":"2023-09-09T20:39:40.282260Z","shell.execute_reply.started":"2023-09-09T20:39:40.270016Z","shell.execute_reply":"2023-09-09T20:39:40.281300Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"__main__.test_log_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def unbroadcast(broadcasted : Arr, original : Arr) -> Arr:\n    \n    \"\"\"Sum 'broadcasted' until it has the shape of 'original'.\n    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.\n    \"\"\"\n    \n    # 1) Sum and remove dimensions that were prepended to the front of the original shape.\n    prepended_dims = broadcasted.ndim - original.ndim\n    if prepended_dims > 0:\n        broadcasted = broadcasted.sum(axis = tuple(range(prepended_dims)))\n        \n    # 2) Sum dimensions that were originally 1 back to the size 1 (using keepdims=True).\n    broadcasted_dims = tuple([axis for axis, size in enumerate(original.shape) if size == 1 and broadcasted.shape[axis] > 1])\n    broadcasted = broadcasted.sum(axis = broadcasted_dims, keepdims = True)\n    assert broadcasted.shape == original.shape\n    return broadcasted\n\ntest_unbroadcast(unbroadcast)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:40.484262Z","iopub.execute_input":"2023-09-09T20:39:40.485287Z","iopub.status.idle":"2023-09-09T20:39:40.495376Z","shell.execute_reply.started":"2023-09-09T20:39:40.485249Z","shell.execute_reply":"2023-09-09T20:39:40.494279Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"__main__.test_unbroadcast passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def multiply_back0(grad_out : Arr, out : Arr, x : Arr,\n                  y : Union[Arr, float]) -> Arr:\n    \"\"\"Backwards function for x * y wrt argument 0 aka x.\"\"\"\n    op = unbroadcast(grad_out * y, x)\n    return op\n    \n\ndef multiply_back1(grad_out : Arr, out : Arr, x : Union[Arr, float],\n                  y : Arr) -> Arr:\n    \"\"\"Backwards function for x * y wrt argument 1 aka y.\"\"\"\n    op = unbroadcast(grad_out * x, y)\n    return op\n\ntest_multiply_back(multiply_back0, multiply_back1)\ntest_multiply_back_float(multiply_back0, multiply_back1)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:40.740756Z","iopub.execute_input":"2023-09-09T20:39:40.741158Z","iopub.status.idle":"2023-09-09T20:39:40.750585Z","shell.execute_reply.started":"2023-09-09T20:39:40.741125Z","shell.execute_reply":"2023-09-09T20:39:40.749746Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"__main__.test_multiply_back passed in 0.00s.\n__main__.test_multiply_back_float passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def forward_and_back(a : Arr, b : Arr, c : Arr) -> tuple[Arr, Arr, Arr]:\n    \n    \"\"\"\n    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da, dg/db, and dg/dc\n    \"\"\"\n    d = a * b\n    e = np.log(c)\n    f = d * e\n    g = np.log(f)\n    \n    dg_df = log_back(1, g, f)\n    df_dd = multiply_back0(dg_df, f, d, e)\n    df_de = multiply_back1(dg_df, f, d, e)\n    dg_da = multiply_back0(df_dd, d, a, b)\n    dg_db = multiply_back1(df_dd, d, a, b)\n    dg_dc = log_back(df_de, f, c)\n    return dg_da, dg_db, dg_dc\n\ntest_forward_and_back(forward_and_back)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:41.010244Z","iopub.execute_input":"2023-09-09T20:39:41.011217Z","iopub.status.idle":"2023-09-09T20:39:41.020444Z","shell.execute_reply.started":"2023-09-09T20:39:41.011183Z","shell.execute_reply":"2023-09-09T20:39:41.019493Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"__main__.test_forward_and_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass Recipe:\n    \"\"\"Extra information necessary to run backpropagation. You don't need to modify this.\"\"\"\n\n    func: Callable\n    \"The 'inner' NumPy function that does the actual forward computation.\"\n    args: tuple\n    \"The input arguments passed to func.\"\n    kwargs: dict[str, Any]\n    \"Keyword arguments passed to func. To keep things simple today, we aren't going to backpropagate with respect to these.\"\n    parents: dict[int, \"Tensor\"]\n    \"Map from positional argument index to the Tensor at that position, in order to be able to pass gradients back along the computational graph.\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:41.268480Z","iopub.execute_input":"2023-09-09T20:39:41.269473Z","iopub.status.idle":"2023-09-09T20:39:41.275826Z","shell.execute_reply.started":"2023-09-09T20:39:41.269439Z","shell.execute_reply":"2023-09-09T20:39:41.274840Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class BackwardFuncLookup:\n    \n    def __init__(self) -> None:\n        self.back_funcs : defaultdict[Callable, defaultdict[int, Callable]] = defaultdict(dict)\n    \n    def add_back_func(self, forward_fn : Callable,\n                     arg_position : int, back_fn : Callable) -> None:\n        self.back_funcs[forward_fn][arg_position] = back_fn\n    \n    def get_back_func(self, forward_fn : Callable,\n                     arg_position: int) -> Callable:\n        return self.back_funcs[forward_fn][arg_position]\n    \ntest_back_func_lookup(BackwardFuncLookup)\nBACK_FUNCS = BackwardFuncLookup()\nBACK_FUNCS.add_back_func(np.log, 0, log_back)\nBACK_FUNCS.add_back_func(np.multiply, 0, multiply_back0)\nBACK_FUNCS.add_back_func(np.multiply, 1, multiply_back1)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:41.586316Z","iopub.execute_input":"2023-09-09T20:39:41.587021Z","iopub.status.idle":"2023-09-09T20:39:41.596840Z","shell.execute_reply.started":"2023-09-09T20:39:41.586984Z","shell.execute_reply":"2023-09-09T20:39:41.595586Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"__main__.test_back_func_lookup passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"class Tensor:\n    \"\"\"\n    A drop-in replacement for torch.Tensor supporting a subset of features.\n    \"\"\"\n\n    array: Arr\n    \"The underlying array. Can be shared between multiple Tensors.\"\n    requires_grad: bool\n    \"If True, calling functions or methods on this tensor will track relevant data for backprop.\"\n    grad: Optional[\"Tensor\"]\n    \"Backpropagation will accumulate gradients into this field.\"\n    recipe: Optional[Recipe]\n    \"Extra information necessary to run backpropagation.\"\n\n    def __init__(self, array: Union[Arr, list], requires_grad=False):\n        self.array = array if isinstance(array, Arr) else np.array(array)\n        self.requires_grad = requires_grad\n        self.grad = None\n        self.recipe = None\n        \"If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs).\"\n\n    def __neg__(self) -> \"Tensor\":\n        return negative(self)\n\n    def __add__(self, other) -> \"Tensor\":\n        return add(self, other)\n\n    def __radd__(self, other) -> \"Tensor\":\n        return add(other, self)\n\n    def __sub__(self, other) -> \"Tensor\":\n        return subtract(self, other)\n\n    def __rsub__(self, other):\n        return subtract(other, self)\n\n    def __mul__(self, other) -> \"Tensor\":\n        return multiply(self, other)\n\n    def __rmul__(self, other):\n        return multiply(other, self)\n\n    def __truediv__(self, other):\n        return true_divide(self, other)\n\n    def __rtruediv__(self, other):\n        return true_divide(self, other)\n\n    def __matmul__(self, other):\n        return matmul(self, other)\n\n    def __rmatmul__(self, other):\n        return matmul(other, self)\n\n    def __eq__(self, other):\n        return eq(self, other)\n\n    def __repr__(self) -> str:\n        return f\"Tensor({repr(self.array)}, requires_grad={self.requires_grad})\"\n\n    def __len__(self) -> int:\n        if self.array.ndim == 0:\n            raise TypeError\n        return self.array.shape[0]\n\n    def __hash__(self) -> int:\n        return id(self)\n\n    def __getitem__(self, index) -> \"Tensor\":\n        return getitem(self, index)\n\n    def add_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n        add_(self, other, alpha=alpha)\n        return self\n\n    @property\n    def T(self) -> \"Tensor\":\n        return permute(self)\n\n    def item(self):\n        return self.array.item()\n\n    def sum(self, dim=None, keepdim=False):\n        return sum(self, dim=dim, keepdim=keepdim)\n\n    def log(self):\n        return log(self)\n\n    def exp(self):\n        return exp(self)\n\n    def reshape(self, new_shape):\n        return reshape(self, new_shape)\n\n    def expand(self, new_shape):\n        return expand(self, new_shape)\n\n    def permute(self, dims):\n        return permute(self, dims)\n\n    def maximum(self, other):\n        return maximum(self, other)\n\n    def relu(self):\n        return relu(self)\n\n    def argmax(self, dim=None, keepdim=False):\n        return argmax(self, dim=dim, keepdim=keepdim)\n\n    def uniform_(self, low: float, high: float) -> \"Tensor\":\n        self.array[:] = np.random.uniform(low, high, self.array.shape)\n        return self\n\n    def backward(self, end_grad: Union[Arr, \"Tensor\", None] = None) -> None:\n        if isinstance(end_grad, Arr):\n            end_grad = Tensor(end_grad)\n        return backprop(self, end_grad)\n\n    def size(self, dim: Optional[int] = None):\n        if dim is None:\n            return self.shape\n        return self.shape[dim]\n\n    @property\n    def shape(self):\n        return self.array.shape\n\n    @property\n    def ndim(self):\n        return self.array.ndim\n\n    @property\n    def is_leaf(self):\n        \"\"\"Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html\"\"\"\n        if self.requires_grad and self.recipe and self.recipe.parents:\n            return False\n        return True\n\n    def __bool__(self):\n        if np.array(self.shape).prod() != 1:\n            raise RuntimeError(\"bool value of Tensor with more than one value is ambiguous\")\n        return bool(self.item())\n\n\ndef empty(*shape: int) -> Tensor:\n    \"\"\"Like torch.empty.\"\"\"\n    return Tensor(np.empty(shape))\n\n\ndef zeros(*shape: int) -> Tensor:\n    \"\"\"Like torch.zeros.\"\"\"\n    return Tensor(np.zeros(shape))\n\n\ndef arange(start: int, end: int, step=1) -> Tensor:\n    \"\"\"Like torch.arange(start, end).\"\"\"\n    return Tensor(np.arange(start, end, step=step))\n\n\ndef tensor(array: Arr, requires_grad=False) -> Tensor:\n    \"\"\"Like torch.tensor.\"\"\"\n    return Tensor(array, requires_grad=requires_grad)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:41.948761Z","iopub.execute_input":"2023-09-09T20:39:41.949707Z","iopub.status.idle":"2023-09-09T20:39:41.978095Z","shell.execute_reply.started":"2023-09-09T20:39:41.949669Z","shell.execute_reply":"2023-09-09T20:39:41.976905Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def log_forward(x : Tensor) -> Tensor:\n    output = Tensor(np.log(x.array))\n    if grad_tracking_enabled and (x.requires_grad or x.recipe is not None):\n        output.recipe = Recipe(np.log, (x.array,), {}, {0:x})\n        output.requires_grad = True\n        \n    return output\n    \n\nlog = log_forward\ntest_log(Tensor, log_forward)\ntest_log_no_grad(Tensor, log_forward)\na = Tensor([1], requires_grad = True)\ngrad_tracking_enabled = False\nb = log_forward(a)\ngrad_tracking_enabled = True\nassert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\nassert b.recipe is None, \"should not create recipe if grad tracking globally disabled\"\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:42.310039Z","iopub.execute_input":"2023-09-09T20:39:42.310455Z","iopub.status.idle":"2023-09-09T20:39:42.320049Z","shell.execute_reply.started":"2023-09-09T20:39:42.310424Z","shell.execute_reply":"2023-09-09T20:39:42.318787Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"__main__.test_log passed in 0.00s.\n__main__.test_log_no_grad passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def multiply_forward(a : Union[Tensor, int], b : Union[Tensor, int]) -> Tensor:\n    assert isinstance(a, Tensor) or isinstance(b, Tensor)\n    a_arr = a.array if isinstance(a, Tensor) else a\n    b_arr = b.array if isinstance(b, Tensor) else b\n    out_arr = a_arr * b_arr\n    output = Tensor(out_arr)\n    assert isinstance(out_arr, np.ndarray)\n    if grad_tracking_enabled and (\n        isinstance(a, Tensor)\n        and (a.requires_grad or a.recipe is not None)\n        or isinstance(b, Tensor)\n        and (b.requires_grad or b.recipe is not None)\n    ):\n        parents = {}\n        if isinstance(a, Tensor):\n            parents[0] = a\n        if isinstance(b, Tensor):\n            parents[1] = b\n            \n        output.recipe = Recipe(np.multiply, (a_arr, b_arr), {}, parents)\n        output.requires_grad = True\n        \n    return output\n    \n\n\nmultiply = multiply_forward\ntest_multiply(Tensor, multiply_forward)\ntest_multiply_no_grad(Tensor, multiply_forward)\ntest_multiply_float(Tensor, multiply_forward)\na = Tensor([2], requires_grad=True)\nb = Tensor([3], requires_grad=True)\ngrad_tracking_enabled = False\nb = multiply_forward(a, b)\ngrad_tracking_enabled = True\nassert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\nassert b.recipe is None, \"should not create recipe if grad tracking globally disabled\"","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:42.686053Z","iopub.execute_input":"2023-09-09T20:39:42.686445Z","iopub.status.idle":"2023-09-09T20:39:42.700355Z","shell.execute_reply.started":"2023-09-09T20:39:42.686413Z","shell.execute_reply":"2023-09-09T20:39:42.699514Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"__main__.test_multiply passed in 0.00s.\n__main__.test_multiply_no_grad passed in 0.00s.\n__main__.test_multiply_float passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:\n    \"\"\"\n    numpy_func: function. It takes any number of positional arguments, some of which may be NumPy arrays, and any number of keyword arguments which we aren't allowing to be NumPy arrays at present. It returns a single NumPy array.\n    is_differentiable: if True, numpy_func is differentiable with respect to some input argument, so we may need to track information in a Recipe. If False, we definitely don't need to track information.\n\n    Return: function. It has the same signature as numpy_func, except wherever there was a NumPy array, this has a Tensor instead.\n    \"\"\"\n\n    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:\n        \n        args_arr = tuple([arg.array if isinstance(arg, Tensor) else arg for arg in args])\n        for k, v in kwargs.items():\n            if isinstance(v, Tensor):\n                raise ValueError(f\"Keyword tensors not supported, got key: {k}, value: {v}\")\n        \n        out = Tensor(numpy_func(*args_arr, **kwargs))\n        if grad_tracking_enabled and is_differentiable:\n            parents = {\n                i : arg\n                for i, arg in enumerate(args)\n                if isinstance(arg, Tensor) and (arg.requires_grad or arg.recipe is not None)\n            }\n            if parents:\n                out.recipe = Recipe(numpy_func, args_arr, kwargs, parents)\n                out.requires_grad = True\n                \n        return out\n\n    return tensor_func\n\nlog = wrap_forward_fn(np.log)\nmultiply = wrap_forward_fn(np.multiply)\ntest_log(Tensor, log)\ntest_log_no_grad(Tensor, log)\ntest_multiply(Tensor, multiply)\ntest_multiply_no_grad(Tensor, multiply)\ntest_multiply_float(Tensor, multiply)\n\ntry:\n    log(x = Tensor([100]))\nexcept Exception as e:\n    print(\"Got a nice exception as intended:\")\n    print(e)\nelse:\n    assert False, \"Passing tensor by keyword should raise some informative exception.\"\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:43.121309Z","iopub.execute_input":"2023-09-09T20:39:43.121980Z","iopub.status.idle":"2023-09-09T20:39:43.136095Z","shell.execute_reply.started":"2023-09-09T20:39:43.121945Z","shell.execute_reply":"2023-09-09T20:39:43.134814Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"__main__.test_log passed in 0.00s.\n__main__.test_log_no_grad passed in 0.00s.\n__main__.test_multiply passed in 0.00s.\n__main__.test_multiply_no_grad passed in 0.00s.\n__main__.test_multiply_float passed in 0.00s.\nGot a nice exception as intended:\nKeyword tensors not supported, got key: x, value: Tensor(array([100]), requires_grad=False)\n","output_type":"stream"}]},{"cell_type":"code","source":"class Node(Protocol):\n    \"\"\"\n    A protocol defining the Node's interface in topological sort. Any object will do!\n    \"\"\"\n\n\nclass ChildrenGetter(Protocol):\n    \"\"\"A protocol defining the get_children_fns passed to topological sort, to get the node's children\"\"\"\n\n    def __call__(self, node: Any) -> list[Any]:\n        \"\"\"Get the given node's children, returning a list of nodes\"\"\"\n        ...\n\n\ndef topological_sort(node: Node, get_children_fn: ChildrenGetter) -> list[Any]:\n    \"\"\"\n    Return a list of node's descendants in reverse topological order from future to past.\n    \"\"\"\n    \n    visited: set[Node] = set()\n    solution: set[Node] = set()\n    result: list[Node] = []\n        \n    def visit(curr : Node):\n        \n        if curr in solution:\n            return\n        if curr in visited:\n            raise ValueError('Detected cyclic graph')\n        visited.add(curr)\n        \n        for next in get_children_fn(curr):\n            visit(next)\n            \n        visited.remove(curr)\n        solution.add(curr)\n        result.append(curr)\n        \n    visit(node)\n    return result\n        \n\"\"\"\ntest_topological_sort_linked_list(topological_sort)\ntest_topological_sort_branching(topological_sort)\ntest_topological_sort_rejoining(topological_sort)\ntest_topological_sort_cyclic(topological_sort)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:44.204154Z","iopub.execute_input":"2023-09-09T20:39:44.204581Z","iopub.status.idle":"2023-09-09T20:39:44.219052Z","shell.execute_reply.started":"2023-09-09T20:39:44.204547Z","shell.execute_reply":"2023-09-09T20:39:44.217942Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\ntest_topological_sort_linked_list(topological_sort)\\ntest_topological_sort_branching(topological_sort)\\ntest_topological_sort_rejoining(topological_sort)\\ntest_topological_sort_cyclic(topological_sort)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def sorted_computational_graph(node: Tensor) -> list[Tensor]:\n    \"\"\"\n    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, in reverse topological order\n    \"\"\"\n    \n    def get_parents(node : Tensor) -> list[Tensor]:\n        if node.recipe is None:\n            return []\n        return list(node.recipe.parents.values())\n    \n    return list(reversed(topological_sort(node, get_parents)))\n\na = Tensor([1], requires_grad = True)\nb = Tensor([2], requires_grad = True)\nc = Tensor([3], requires_grad = True)\nd = a * b\ne = c.log()\nf = d * e\ng = f.log()\nname_lookup = {a: \"a\", b: \"b\", c: \"c\", d: \"d\", e: \"e\", f: \"f\", g: \"g\"}\nprint([name_lookup[t] for t in sorted_computational_graph(g)])\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:44.724641Z","iopub.execute_input":"2023-09-09T20:39:44.725760Z","iopub.status.idle":"2023-09-09T20:39:44.735695Z","shell.execute_reply.started":"2023-09-09T20:39:44.725715Z","shell.execute_reply":"2023-09-09T20:39:44.734625Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"['g', 'f', 'e', 'c', 'd', 'b', 'a']\n","output_type":"stream"}]},{"cell_type":"code","source":"def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:\n    \"\"\"Accumulate gradients in the grad field of each leaf node.\n    \n    tensor.backward() is equivalent to backprop(tensor).\n    \n    end_node: the rightmost node in the computation graph. If it contains more than one element, end_grad must be provided.\n    end_grad: A tensor of the same shape as end_node. Set to 1 if not specified and end_node has only one element. \n    \"\"\"\n    \n    if np.array(end_node.shape).prod() > 1 and end_grad is None:\n        raise RuntimeError(\"backprop from non-scalar tensors requires end_grad\")\n    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array\n    grads: dict[Tensor, Arr] = {end_node: end_grad_arr}\n    \n    for node in sorted_computational_graph(end_node):\n        last_grad = grads.pop(node)\n        if node.is_leaf:\n            if node.grad is None:\n                node.grad = Tensor(last_grad)\n            else:\n                node.grad += last_grad\n        \n        if node.recipe is None:\n            continue\n            \n        for argnum, parent in node.recipe.parents.items():\n            back_fn = BACK_FUNCS.get_back_func(node.recipe.func, argnum)\n            in_grad = back_fn(last_grad, node.array, *node.recipe.args, **node.recipe.kwargs)\n            cur_pgrad = grads.get(parent)\n            if cur_pgrad is None:\n                grads[parent] = in_grad\n            else:\n                grads[parent] += in_grad\n        \n\ntest_backprop(Tensor)\ntest_backprop_branching(Tensor)\ntest_backprop_float_arg(Tensor)\ntest_backprop_requires_grad_false(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:44.956632Z","iopub.execute_input":"2023-09-09T20:39:44.957212Z","iopub.status.idle":"2023-09-09T20:39:44.969539Z","shell.execute_reply.started":"2023-09-09T20:39:44.957181Z","shell.execute_reply":"2023-09-09T20:39:44.968352Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"__main__.test_backprop passed in 0.00s.\n__main__.test_backprop_branching passed in 0.00s.\n__main__.test_backprop_float_arg passed in 0.00s.\n__main__.test_backprop_requires_grad_false passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def _argmax(x:Arr, dim = None, keepdim = False):\n    \"\"\"Like torch.argmax\"\"\"\n    return np.argmax(x, axis = dim, keepdims = keepdim)\n\nargmax = wrap_forward_fn(_argmax, is_differentiable = False)\neq = wrap_forward_fn(np.equal, is_differentiable = False)\n\na = Tensor([1.0, 0.0, 3.0, 4.0], requires_grad = False)\nb = a.argmax()\nassert not b.requires_grad\nassert b.recipe is None\nassert b.item() == 3","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:45.217869Z","iopub.execute_input":"2023-09-09T20:39:45.218498Z","iopub.status.idle":"2023-09-09T20:39:45.227110Z","shell.execute_reply.started":"2023-09-09T20:39:45.218457Z","shell.execute_reply":"2023-09-09T20:39:45.225927Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n    \"\"\"Backward function for f(x) = -x elementwise.\"\"\"\n    return -np.ones_like(x) * grad_out\n\nnegative = wrap_forward_fn(np.negative)\nBACK_FUNCS.add_back_func(np.negative, 0, negative_back)\ntest_negative_back(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:45.423172Z","iopub.execute_input":"2023-09-09T20:39:45.423565Z","iopub.status.idle":"2023-09-09T20:39:45.431135Z","shell.execute_reply.started":"2023-09-09T20:39:45.423532Z","shell.execute_reply":"2023-09-09T20:39:45.430041Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"__main__.test_negative_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n    return out * grad_out\n\nexp = wrap_forward_fn(np.exp)\nBACK_FUNCS.add_back_func(np.exp, 0, exp_back)\ntest_exp_back(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:45.722103Z","iopub.execute_input":"2023-09-09T20:39:45.722690Z","iopub.status.idle":"2023-09-09T20:39:45.728973Z","shell.execute_reply.started":"2023-09-09T20:39:45.722654Z","shell.execute_reply":"2023-09-09T20:39:45.728097Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"__main__.test_exp_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr:\n    return grad_out.reshape(x.shape)\n\nreshape = wrap_forward_fn(np.reshape)\nBACK_FUNCS.add_back_func(np.reshape, 0, reshape_back)\ntest_reshape_back(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:46.080995Z","iopub.execute_input":"2023-09-09T20:39:46.081672Z","iopub.status.idle":"2023-09-09T20:39:46.088328Z","shell.execute_reply.started":"2023-09-09T20:39:46.081639Z","shell.execute_reply":"2023-09-09T20:39:46.087421Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"__main__.test_reshape_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> Arr:\n    new_axes = np.argsort(axes)\n    return grad_out.transpose(new_axes)\n\nBACK_FUNCS.add_back_func(np.transpose, 0, permute_back)\npermute = wrap_forward_fn(np.transpose)\ntest_permute_back(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:46.490410Z","iopub.execute_input":"2023-09-09T20:39:46.491098Z","iopub.status.idle":"2023-09-09T20:39:46.500278Z","shell.execute_reply.started":"2023-09-09T20:39:46.491050Z","shell.execute_reply":"2023-09-09T20:39:46.499058Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"__main__.test_permute_back passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def expand_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr:\n    return unbroadcast(grad_out, x)\n\ndef _expand(x: Arr, new_shape) -> Arr:\n    \"\"\"Like torch.expand, calling np.broadcast_to internally.\n    \n    Note torch.expand supports -1 for a dimension size meaning \"don't change the size\".\n    np.broadcast_to does not natively support this.\n    \"\"\"\n    \n    shape_diff = len(new_shape) - x.ndim\n    new_expanded_shape = tuple([x.shape[i - shape_diff] if s == -1 else s for i, s in enumerate(new_shape)])\n    return np.broadcast_to(x, new_expanded_shape)\n\nexpand = wrap_forward_fn(_expand)\nBACK_FUNCS.add_back_func(_expand, 0, expand_back)\ntest_expand(Tensor)\ntest_expand_negative_length(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:47.044684Z","iopub.execute_input":"2023-09-09T20:39:47.045101Z","iopub.status.idle":"2023-09-09T20:39:47.055137Z","shell.execute_reply.started":"2023-09-09T20:39:47.045069Z","shell.execute_reply":"2023-09-09T20:39:47.053986Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"__main__.test_expand passed in 0.00s.\n__main__.test_expand_negative_length passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def _coerce_dim(x, dim: Union[None, int, tuple[int]]) -> tuple[int]:\n    if dim is None:\n        return tuple(range(x.ndim))\n    elif isinstance(dim, int):\n        return tuple([dim])\n    return dim\n\ndef sum_back(grad_out: Arr, out: Arr, x: Arr,\n            dim = None, keepdim = False):\n    dim = _coerce_dim(x, dim)\n    if not keepdim:\n        unsqueezed = tuple([1 if d in dim else size for d, size in enumerate(x.shape)])\n        grad_out = grad_out.reshape(unsqueezed)\n    return np.broadcast_to(grad_out, x.shape)\n    \n\ndef _sum(x: Arr, dim = None, keepdim = False) -> Arr:\n    \"\"\"Like torch.sum, calling np.sum internally.\"\"\"\n    return x.sum(axis = dim, keepdims = keepdim)\n\nsum = wrap_forward_fn(_sum)\nBACK_FUNCS.add_back_func(_sum, 0, sum_back)\ntest_sum_keepdim_false(Tensor)\ntest_sum_keepdim_true(Tensor)\ntest_sum_dim_none(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:47.581416Z","iopub.execute_input":"2023-09-09T20:39:47.581840Z","iopub.status.idle":"2023-09-09T20:39:47.594000Z","shell.execute_reply.started":"2023-09-09T20:39:47.581807Z","shell.execute_reply":"2023-09-09T20:39:47.592776Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"__main__.test_sum_keepdim_false passed in 0.00s.\n__main__.test_sum_keepdim_true passed in 0.00s.\n__main__.test_sum_dim_none passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"Index = Union[int, tuple[int, ...], tuple[Arr], tuple[Tensor]]\n\ndef _coerce_index(item: Union[int, Arr, Tensor]) -> Union[int, Arr]:\n    \n    if isinstance(item, Arr):\n        return item\n    if isinstance(item, Tensor):\n        return item.array\n    return item\n\ndef _getitem(x: Arr, index: Index) -> Arr:\n    \"\"\"Like x[index] when x is a torch.Tensor.\"\"\"\n    if isinstance(index, int):\n        return x[index]\n    else:\n        new_idx = tuple([_coerce_index(idx) for idx in index])\n        return x[new_idx]\n\ndef getitem_back(grad_out: Arr, out: Arr, x: Arr,\n                index: Index):\n    \"\"\"Backwards function for _getitem.\n    \n    Hint: use np.add.at(a, indices, b)\n    \"\"\"\n    new_arr = np.zeros_like(x)\n    if isinstance(index, int):\n        np.add.at(new_arr, index, grad_out)\n    else:\n        new_idx = tuple([_coerce_index(idx) for idx in index])\n        np.add.at(new_arr, new_idx, grad_out)\n    return new_arr\n\ngetitem = wrap_forward_fn(_getitem)\nBACK_FUNCS.add_back_func(_getitem, 0, getitem_back)\ntest_getitem_int(Tensor)\ntest_getitem_tuple(Tensor)\ntest_getitem_integer_array(Tensor)\ntest_getitem_integer_tensor(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:48.039108Z","iopub.execute_input":"2023-09-09T20:39:48.039722Z","iopub.status.idle":"2023-09-09T20:39:48.053873Z","shell.execute_reply.started":"2023-09-09T20:39:48.039687Z","shell.execute_reply":"2023-09-09T20:39:48.052643Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"__main__.test_getitem_int passed in 0.00s.\n__main__.test_getitem_tuple passed in 0.00s.\n__main__.test_getitem_integer_array passed in 0.00s.\n__main__.test_getitem_integer_tensor passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"add = wrap_forward_fn(np.add)\nsubtract = wrap_forward_fn(np.subtract)\ntrue_divide = wrap_forward_fn(np.true_divide)\nBACK_FUNCS.add_back_func(np.add, 0, lambda grad_out, out, x, y : unbroadcast(grad_out, x))\nBACK_FUNCS.add_back_func(np.add, 1, lambda grad_out, out, x, y : unbroadcast(grad_out, y))\nBACK_FUNCS.add_back_func(np.subtract, 0, lambda grad_out, out, x, y : unbroadcast(grad_out, x))\nBACK_FUNCS.add_back_func(np.subtract, 1, lambda grad_out, out, x, y : unbroadcast(-grad_out, y))\nBACK_FUNCS.add_back_func(np.true_divide, 0, lambda grad_out, out, x, y : unbroadcast(grad_out * (1 / y), x))\nBACK_FUNCS.add_back_func(np.true_divide, 1, lambda grad_out, out, x, y : unbroadcast(grad_out * (x / -np.square(y)), y))\n\ntest_add_broadcasted(Tensor)\ntest_subtract_broadcasted(Tensor)\ntest_truedivide_broadcasted(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:48.559940Z","iopub.execute_input":"2023-09-09T20:39:48.560314Z","iopub.status.idle":"2023-09-09T20:39:48.571953Z","shell.execute_reply.started":"2023-09-09T20:39:48.560276Z","shell.execute_reply":"2023-09-09T20:39:48.570687Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"__main__.test_add_broadcasted passed in 0.00s.\n__main__.test_subtract_broadcasted passed in 0.00s.\n__main__.test_truedivide_broadcasted passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_(x: Tensor, other: Tensor, alpha: float = 1.0) -> Tensor:\n    \"\"\"Like torch.add_. Compute x += other * alpha in-place and return tensor.\"\"\"\n    np.add(x.array, other.array * alpha, out = x.array)\n    \ndef safe_example():\n    \"\"\"This example should work properly.\"\"\"\n    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad = True)\n    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad = True)\n    a.add_(b)\n    c = a * b\n    c.sum().backward()\n    assert a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0])\n    assert b.grad is not None and np.allclose(b.grad.array, [2.0, 4.0, 6.0, 8.0])\n    \ndef unsafe_example():\n    \"\"\"This example is expected to compute the wrong gradients.\"\"\"\n    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad = True)\n    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad = True)\n    c = a * b\n    a.add_(b)\n    c.sum().backward()\n    if a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0]):\n        print(\"Grad wrt a is OK!\")\n    else:\n        print(\"Grad wrt a is WRONG!\")\n        \n    if b.grad is not None and np.allclose(b.grad.array, [0.0, 1.0, 2.0, 3.0]):\n        print(\"Grad wrt b is OK!\")\n    else:\n        print(\"Grad wrt b is WRONG!\")\n        \nsafe_example()\nunsafe_example()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:49.121085Z","iopub.execute_input":"2023-09-09T20:39:49.122153Z","iopub.status.idle":"2023-09-09T20:39:49.135739Z","shell.execute_reply.started":"2023-09-09T20:39:49.122119Z","shell.execute_reply":"2023-09-09T20:39:49.134625Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Grad wrt a is OK!\nGrad wrt b is WRONG!\n","output_type":"stream"}]},{"cell_type":"code","source":"a = Tensor([0, 1, 2, 3], requires_grad=True)\n(a * 2).sum().backward()\nb = Tensor([0, 1, 2, 3], requires_grad=True)\n(2 * b).sum().backward()\nassert a.grad is not None\nassert b.grad is not None\nassert np.allclose(a.grad.array, b.grad.array)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:49.673367Z","iopub.execute_input":"2023-09-09T20:39:49.673760Z","iopub.status.idle":"2023-09-09T20:39:49.681151Z","shell.execute_reply.started":"2023-09-09T20:39:49.673728Z","shell.execute_reply":"2023-09-09T20:39:49.680040Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def maximum_back0(grad_out: Arr, out: Arr,\n                 x: Arr, y: Arr):\n    \"\"\"Backwards function for max(x, y) wrt x.\"\"\"\n    out = grad_out.copy()\n    out = np.where(x == y, grad_out / 2, out)\n    out = np.where(x < y, 0.0, out)\n    return unbroadcast(out, x)\n\n\ndef maximum_back1(grad_out: Arr, out: Arr,\n                 x: Arr, y: Arr):\n    \"\"\"Backwards function for max(x, y) wrt y.\"\"\"\n    out = grad_out.copy()\n    out = np.where(x == y, grad_out / 2, out)\n    out = np.where(x > y, 0.0, out)\n    return unbroadcast(out, y)\n\nmaximum = wrap_forward_fn(np.maximum)\nBACK_FUNCS.add_back_func(np.maximum, 0, maximum_back0)\nBACK_FUNCS.add_back_func(np.maximum, 1, maximum_back1)\ntest_maximum(Tensor)\ntest_maximum(Tensor)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:50.142844Z","iopub.execute_input":"2023-09-09T20:39:50.143218Z","iopub.status.idle":"2023-09-09T20:39:50.155743Z","shell.execute_reply.started":"2023-09-09T20:39:50.143190Z","shell.execute_reply":"2023-09-09T20:39:50.154384Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"__main__.test_maximum passed in 0.00s.\n__main__.test_maximum passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def relu(x: Tensor) -> Tensor:\n    \"\"\"Like torch.nn.function.relu(x, inplace=False).\"\"\"\n    return maximum(0.0, x)\n\ntest_relu(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:39:50.524423Z","iopub.execute_input":"2023-09-09T20:39:50.524810Z","iopub.status.idle":"2023-09-09T20:39:50.531622Z","shell.execute_reply.started":"2023-09-09T20:39:50.524755Z","shell.execute_reply":"2023-09-09T20:39:50.530309Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"__main__.test_relu passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def _matmul2d(x: Arr, y: Arr) -> Arr:\n    \"\"\"Matrix multiply restricted to the case where both inputs are exactly 2D.\"\"\"\n    return x @ y\n\ndef matmul2d_back0(grad_out: Arr, out: Arr,\n                  x: Arr, y: Arr) -> Arr:\n    return grad_out @ np.transpose(y)\n\ndef matmul2d_back1(grad_out: Arr, out: Arr,\n                  x: Arr, y: Arr) -> Arr:\n    return np.transpose(x) @ grad_out\n\nmatmul = wrap_forward_fn(_matmul2d)\nBACK_FUNCS.add_back_func(_matmul2d, 0, matmul2d_back0)\nBACK_FUNCS.add_back_func(_matmul2d, 1, matmul2d_back1)\ntest_matmul2d(Tensor)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:46:13.722522Z","iopub.execute_input":"2023-09-09T20:46:13.722922Z","iopub.status.idle":"2023-09-09T20:46:13.732946Z","shell.execute_reply.started":"2023-09-09T20:46:13.722887Z","shell.execute_reply":"2023-09-09T20:46:13.731861Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"__main__.test_matmul2d passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"class Parameter(Tensor):\n    def __init__(self, tensor : Tensor, requires_grad = True):\n        \"\"\"Share the array with the provided tensor.\"\"\"\n        return super().__init__(tensor.array, requires_grad = requires_grad)\n    \n    def __repr__(self):\n        return f\"Parameter containing:\\n{super().__repr__()}\"\n    \nx = Tensor([1.0, 2.0, 3.0])\np = Parameter(x)\nassert p.requires_grad\nassert p.array is x.array\nassert repr(p) == \"Parameter containing:\\nTensor(array([1., 2., 3.]), requires_grad=True)\"\nx.add_(Tensor(np.array(2.0)))\nassert np.allclose(\n        p.array, np.array([3.0, 4.0, 5.0])\n    ), \"in-place modifications to the original tensor should affect the parameter\"\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T20:50:53.463028Z","iopub.execute_input":"2023-09-09T20:50:53.463408Z","iopub.status.idle":"2023-09-09T20:50:53.472808Z","shell.execute_reply.started":"2023-09-09T20:50:53.463379Z","shell.execute_reply":"2023-09-09T20:50:53.471559Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class Module:\n    _modules: dict[str, \"Module\"]\n    _parameters: dict[str, Parameter]\n        \n    def __init__(self):\n        self._modules = {}\n        self._parameters = {}\n    \n    def modules(self):\n        \"\"\"Return the direct child modules of this module.\"\"\"\n        return self.__dict__[\"_modules\"].values()\n    \n    def _iter_params(self):\n        yield from self._parameters.values()\n        for child in self._modules.values():\n            yield from child.parameters(recurse=True)\n    \n    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n        \"\"\"Return an iterator over Module parameters.\n        \n        recurse: if True, the iterator includes parameters of submodules, recursively.\n        \"\"\"\n        if recurse:\n            return self._iter_params()\n        else:\n            return iter(self._parameters.values())\n    \n    def __setattr__(self, key: str, val: Any) -> None:\n        \"\"\"\n        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.\n        Otherwise, call the superclass.\n        \"\"\"\n        if isinstance(val, Parameter):\n            self.__dict__[\"_parameters\"][key] = val\n        elif isinstance(val, Module):\n            self.__dict__[\"_modules\"][key] = val\n        else:\n            super().__setattr__(key, val)\n    \n    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:\n        \"\"\"\n        If key is in _parameters or _modules, return the corresponding value.\n        Otherwise, raise KeyError.\n        \"\"\"\n        if key in self.__dict__['_parameters']:\n            return self.__dict__['_parameters'][key]\n        \n        if key in self.__dict__['_modules']:\n            return self.__dict__['_modules'][key]\n        \n        raise KeyError(key)\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n    \n    def forward(self):\n        raise NotImplementedError(\"Subclasses must implement forward!\")\n        \n    def __repr__(self):\n        \n        def _addindent(s_, numSpaces):\n            s = s_.split(\"\\n\")\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(numSpaces * \" \") + line for line in s]\n            s = \"\\n\".join(s)\n            s = first + \"\\n\" + s\n            return s\n\n        child_lines = []\n\n        for key, module in self._modules.items():\n            mod_str = repr(module)\n            mod_str = _addindent(mod_str, 2)\n            child_lines.append(\"(\" + key + \"): \" + mod_str)\n        lines = child_lines\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n    \nclass TestInnerModule(Module):\n    def __init__(self):\n        super().__init__()\n        self.param1 = Parameter(Tensor([1.0]))\n        self.param2 = Parameter(Tensor([2.0]))\n\nclass TestModule(Module):\n    def __init__(self):\n        super().__init__()\n        self.inner = TestInnerModule()\n        self.param3 = Parameter(Tensor([3.0]))\n\nmod = TestModule()\nassert list(mod.modules()) == [mod.inner]\nassert list(mod.parameters()) == [\n    mod.param3,\n    mod.inner.param1,\n    mod.inner.param2,\n], \"parameters should come before submodule parameters\"\nprint(\"Manually verify that the repr looks reasonable:\")\nprint(mod)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:11:41.447108Z","iopub.execute_input":"2023-09-09T21:11:41.447479Z","iopub.status.idle":"2023-09-09T21:11:41.471553Z","shell.execute_reply.started":"2023-09-09T21:11:41.447450Z","shell.execute_reply":"2023-09-09T21:11:41.470518Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Manually verify that the repr looks reasonable:\nTestModule(\n  (inner): TestInnerModule()\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"class Linear(Module):\n    weight: Parameter\n    bias: Optional[Parameter]\n\n    def __init__(self, in_features: int, out_features: int, bias=True):\n        \"\"\"A simple linear (technically, affine) transformation.\n\n        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n        If `bias` is False, set `self.bias` to None.\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        bound = in_features ** -0.5\n        self.weight = Parameter(empty(self.out_features, self.in_features).uniform_(-bound, bound))\n        if bias:\n            self.bias = Parameter(empty(out_features).uniform_(-bound, bound))\n        else:\n            self.bias = None\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        x: shape (*, in_features)\n        Return: shape (*, out_features)\n        \"\"\"\n        out = x @ self.weight.permute((1, 0))\n        if self.bias is not None:\n            out = out + self.bias\n        return out\n\n    def extra_repr(self) -> str:\n        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:18:47.803525Z","iopub.execute_input":"2023-09-09T21:18:47.803984Z","iopub.status.idle":"2023-09-09T21:18:47.814601Z","shell.execute_reply.started":"2023-09-09T21:18:47.803947Z","shell.execute_reply":"2023-09-09T21:18:47.813215Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class MLP(Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = Linear(28 * 28, 64)\n        self.linear2 = Linear(64, 64)\n        self.output = Linear(64, 10)\n\n    def forward(self, x):\n        x = x.reshape((x.shape[0], 28 * 28))\n        x = relu(self.linear1(x))\n        x = relu(self.linear2(x))\n        x = self.output(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:18:56.782448Z","iopub.execute_input":"2023-09-09T21:18:56.783044Z","iopub.status.idle":"2023-09-09T21:18:56.792347Z","shell.execute_reply.started":"2023-09-09T21:18:56.783001Z","shell.execute_reply":"2023-09-09T21:18:56.791063Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:\n    \"\"\"Like torch.nn.functional.cross_entropy with reduction='none'.\n    \n    logits: shape (batch, classes)\n    true_labels: shape (batch, ). Each element is the index of the correct label in the logits.\n    \n    Return: shape(batch, ) containing the per-example loss.\n    \"\"\"\n    batch_size, n_classes = logits.shape\n    true = logits[arange(0, batch_size), true_labels]\n    return -log(exp(true) / exp(logits).sum(1))\n\ntest_cross_entropy(Tensor, cross_entropy)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:29:26.084913Z","iopub.execute_input":"2023-09-09T21:29:26.085311Z","iopub.status.idle":"2023-09-09T21:29:26.094572Z","shell.execute_reply.started":"2023-09-09T21:29:26.085282Z","shell.execute_reply":"2023-09-09T21:29:26.093380Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"__main__.test_cross_entropy passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"class NoGrad:\n    \"\"\"Context manager that disables grad inside the block. Like torch.no_grad.\"\"\"\n\n    was_enabled: bool\n\n    def __enter__(self):\n        global grad_tracking_enabled\n        self.was_enabled = grad_tracking_enabled\n        grad_tracking_enabled = False\n\n    def __exit__(self, type, value, traceback):\n        global grad_tracking_enabled\n        grad_tracking_enabled = self.was_enabled\n\n\ntest_no_grad(Tensor, NoGrad)\ntest_no_grad_nested(Tensor, NoGrad)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:33:37.373671Z","iopub.execute_input":"2023-09-09T21:33:37.374074Z","iopub.status.idle":"2023-09-09T21:33:37.381878Z","shell.execute_reply.started":"2023-09-09T21:33:37.374043Z","shell.execute_reply":"2023-09-09T21:33:37.380644Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"__main__.test_no_grad passed in 0.00s.\n__main__.test_no_grad_nested passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"def visualize(dataloader):\n    \"\"\"Call this if you want to see some of your data.\"\"\"\n    plt.figure(figsize=(12, 12))\n    (sample, sample_labels) = next(iter(dataloader))\n    for i in range(10):\n        plt.subplot(5, 5, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(sample[i, 0], cmap=plt.cm.binary)\n    plt.show()\n    \ndef get_mnist(subsample: Optional[int] = None):\n    \"\"\"Return MNIST data using the provided Tensor class.\"\"\"\n    mnist_train = datasets.MNIST(\"../data\", train=True, download=True)\n    mnist_test = datasets.MNIST(\"../data\", train=False)\n    if subsample is None:\n        subsample = 1\n    print(\"Preprocessing data...\")\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.28,), (0.35,))])\n    train_indexes = range(0, len(mnist_train), subsample)\n    train_reduced = [mnist_train[i] for i in train_indexes]\n    train_tensors = torch.utils.data.TensorDataset(\n        torch.stack([transform(img) for img, label in tqdm(train_reduced, desc=\"Training data\")]),\n        torch.tensor([label for img, label in train_reduced]),\n    )\n\n    test_indexes = range(0, len(mnist_test), subsample)\n    test_reduced = [mnist_test[i] for i in test_indexes]\n    test_tensors = torch.utils.data.TensorDataset(\n        torch.stack([transform(img) for img, label in tqdm(test_reduced, desc=\"Test data\")]),\n        torch.tensor([label for img, label in test_reduced]),\n    )\n\n    train_loader = torch.utils.data.DataLoader(train_tensors, shuffle=True, batch_size=512)\n    test_loader = torch.utils.data.DataLoader(test_tensors, batch_size=512)\n    return train_loader, test_loader\n    \n\nsubsample = 20 if IS_CI else None\n(train_loader, test_loader) = get_mnist(subsample)\n    \nclass SGD:\n    def __init__(self, params: Iterable[Parameter], lr: float):\n        \"\"\"Vanilla SGD with no additional features.\"\"\"\n        self.params = list(params)\n        self.lr = lr\n        self.b = [None for _ in self.params]\n\n    def zero_grad(self) -> None:\n        for p in self.params:\n            p.grad = None\n\n    def step(self) -> None:\n        with NoGrad():\n            for (i, p) in enumerate(self.params):\n                assert isinstance(p.grad, Tensor)\n                p.add_(p.grad, -self.lr)\n                \n\n\ndef train(model, train_loader, optimizer, epoch):\n    for (batch_idx, (data, target)) in enumerate(train_loader):\n        data = Tensor(data.numpy())\n        target = Tensor(target.numpy())\n        optimizer.zero_grad()\n        output = model(data)\n        loss = cross_entropy(output, target).sum() / len(output)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 50 == 0:\n            print(\n                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                    epoch,\n                    batch_idx * len(data),\n                    len(train_loader.dataset),\n                    100.0 * batch_idx / len(train_loader),\n                    loss.item(),\n                )\n            )\n\n\ndef test(model, test_loader):\n    test_loss = 0\n    correct = 0\n    with NoGrad():\n        for (data, target) in test_loader:\n            data = Tensor(data.numpy())\n            target = Tensor(target.numpy())\n            output = model(data)\n            test_loss += cross_entropy(output, target).sum().item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += (pred == target.reshape(pred.shape)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    print(\n        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:37:19.921093Z","iopub.execute_input":"2023-09-09T21:37:19.921752Z","iopub.status.idle":"2023-09-09T21:37:36.632761Z","shell.execute_reply.started":"2023-09-09T21:37:19.921708Z","shell.execute_reply":"2023-09-09T21:37:36.631603Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|| 9912422/9912422 [00:00<00:00, 82366633.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|| 28881/28881 [00:00<00:00, 38639774.74it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|| 1648877/1648877 [00:00<00:00, 19008527.03it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|| 4542/4542 [00:00<00:00, 13406424.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nPreprocessing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training data:   0%|          | 0/60000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926aae186a5b47bf8e8052383439b6f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Test data:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4612676376748ccbb62f21121106d74"}},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 5\nmodel = MLP()\nstart = time.time()\noptimizer = SGD(model.parameters(), 0.01)\nfor epoch in range(num_epochs):\n    train(model, train_loader, optimizer, epoch)\n    test(model, test_loader)\n    optimizer.step()\n    \nprint(f\"Completed in {time.time() - start: .2f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T21:38:55.090287Z","iopub.execute_input":"2023-09-09T21:38:55.090784Z","iopub.status.idle":"2023-09-09T21:39:12.067817Z","shell.execute_reply.started":"2023-09-09T21:38:55.090727Z","shell.execute_reply":"2023-09-09T21:39:12.066331Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.305612\nTrain Epoch: 0 [25600/60000 (42%)]\tLoss: 2.185666\nTrain Epoch: 0 [51200/60000 (85%)]\tLoss: 2.001889\n\nTest set: Average loss: 1.8699, Accuracy: 5055/10000 (51%)\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 1.888215\nTrain Epoch: 1 [25600/60000 (42%)]\tLoss: 1.487993\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 1.169869\n\nTest set: Average loss: 1.0379, Accuracy: 7679/10000 (77%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 1.091422\nTrain Epoch: 2 [25600/60000 (42%)]\tLoss: 0.862458\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.726443\n\nTest set: Average loss: 0.6645, Accuracy: 8418/10000 (84%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.681887\nTrain Epoch: 3 [25600/60000 (42%)]\tLoss: 0.584505\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.535096\n\nTest set: Average loss: 0.5214, Accuracy: 8653/10000 (87%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.579696\nTrain Epoch: 4 [25600/60000 (42%)]\tLoss: 0.533549\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.511112\n\nTest set: Average loss: 0.4510, Accuracy: 8793/10000 (88%)\n\nCompleted in  16.97s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}