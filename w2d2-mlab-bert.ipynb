{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops\n!pip install fancy_einsum\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Union\nimport torch as t\nimport transformers\nfrom einops import rearrange, repeat\nfrom fancy_einsum import einsum\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom typing import Optional, Iterator, cast, TypeVar, Generic, Callable\n\nimport tempfile\nimport os\nimport time\nimport torch as t\nfrom torch import nn\nimport transformers\nimport joblib\nimport requests\nimport logging\nfrom transformers.models.bert.modeling_bert import BertForMaskedLM\nimport http\nfrom functools import wraps","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:19:03.076117Z","iopub.execute_input":"2023-09-24T03:19:03.077465Z","iopub.status.idle":"2023-09-24T03:19:48.007407Z","shell.execute_reply.started":"2023-09-24T03:19:03.077423Z","shell.execute_reply":"2023-09-24T03:19:48.006376Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.6.1\nCollecting fancy_einsum\n  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\nInstalling collected packages: fancy_einsum\nSuccessfully installed fancy_einsum-0.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"DEBUG_TOLERANCES = os.getenv(\"DEBUG_TOLERANCES\")\n\ndef assert_all_equal(actual: t.Tensor, expected: t.Tensor) -> None:\n    \"\"\"Assert that actual and expected are exactly equal (to floating point precision).\"\"\"\n    mask = actual == expected\n    if not mask.all().item():\n        bad = mask.nonzero()\n        msg = f\"Did not match at {len(bad)} indexes: {bad[:10]}{'...' if len(bad) > 10 else ''}\"\n        raise AssertionError(f\"{msg}\\nActual:\\n{actual}\\nExpected:\\n{expected}\")\n        \n    \ndef test_is_equal(actual: t.Tensor, expected: t.Tensor, test_name: str) -> None:\n    try:\n        run_and_report(assert_all_equal, test_name, actual, expected)\n    except AssertionError as e:\n        print(f\"Test failed: {test_name}\")\n        raise e\n\n\ndef assert_shape_equal(actual: t.Tensor, expected: t.Tensor) -> None:\n    if actual.shape != expected.shape:\n        raise AssertionError(f\"expected shape={expected.shape}, got {actual.shape}\")\n        \ndef allclose(actual: t.Tensor, expected: t.Tensor, rtol=1e-4) -> None:\n    assert_shape_equal(actual, expected)\n    left = (actual - expected).abs()\n    right = rtol * expected.abs()\n    num_wrong = (left > right).sum().item()\n    if num_wrong > 0:\n        print(f\"Test failed. Max absolute deviation: {left.max()}\")\n        print(f\"Actual:\\n{actual}\\nExpected:\\n{expected}\")\n        raise AssertionError(f\"allclose failed with {num_wrong} / {left.nelement()} entries outside tolerance\")\n    elif DEBUG_TOLERANCES:\n        print(f\"Test passed with max absolute deviation of {left.max()}\")\n\n\ndef allclose_atol(actual: t.Tensor, expected: t.Tensor, atol: float) -> None:\n    assert_shape_equal(actual, expected)\n    left = (actual - expected).abs()\n    num_wrong = (left > atol).sum().item()\n    if num_wrong > 0:\n        print(f\"Test failed. Max absolute deviation: {left.max()}\")\n        print(f\"Actual:\\n{actual}\\nExpected:\\n{expected}\")\n        raise AssertionError(f\"allclose failed with {num_wrong} / {left.nelement()} entries outside tolerance\")\n    elif DEBUG_TOLERANCES:\n        print(f\"Test passed with max absolute deviation of {left.max()}\")\n        \n\n\ndef allclose_scalar(actual: float, expected: float, rtol=1e-4) -> None:\n    left = abs(actual - expected)\n    right = rtol * abs(expected)\n    wrong = left > right\n    if wrong:\n        raise AssertionError(f\"Test failed. Absolute deviation: {left}\\nActual:\\n{actual}\\nExpected:\\n{expected}\")\n    elif DEBUG_TOLERANCES:\n        print(f\"Test passed with absolute deviation of {left}\")\n\n\ndef allclose_scalar_atol(actual: float, expected: float, atol: float) -> None:\n    left = abs(actual - expected)\n    wrong = left > atol\n    if wrong:\n        raise AssertionError(f\"Test failed. Absolute deviation: {left}\\nActual:\\n{actual}\\nExpected:\\n{expected}\")\n    elif DEBUG_TOLERANCES:\n        print(f\"Test passed with absolute deviation of {left}\")\n        \n        \n\ndef report_success(testname):\n    \"\"\"POST to the server indicating success at the given test.\n\n    Used to help the TAs know how long each section takes to complete.\n    \"\"\"\n    server = os.environ.get(\"MLAB_SERVER\")\n    email = os.environ.get(\"MLAB_EMAIL\")\n    if server:\n        if email:\n            r = requests.post(\n                server + \"/api/report_success\",\n                json=dict(email=email, testname=testname),\n            )\n            if r.status_code != http.HTTPStatus.NO_CONTENT:\n                raise ValueError(f\"Got status code from server: {r.status_code}\")\n        else:\n            raise ValueError(f\"Server set to {server} but no MLAB_EMAIL set!\")\n    else:\n        if email:\n            raise ValueError(f\"Email set to {email} but no MLAB_SERVER set!\")\n        else:\n            return  # local dev, do nothing\n\n\n# Map from qualified name \"test_w2d3.test_unidirectional_attn\" to whether this test was passed in the current interpreter session\n# Note this can get clobbered during autoreload\nTEST_FN_PASSED = {}\n\n\ndef report(test_func):\n    name = f\"{test_func.__module__}.{test_func.__name__}\"\n    # This can happen when using autoreload, so don't complain about it.\n    # if name in TEST_FN_PASSED:\n    #     raise KeyError(f\"Already registered: {name}\")\n    TEST_FN_PASSED[name] = False\n\n    @wraps(test_func)\n    def wrapper(*args, **kwargs):\n        return run_and_report(test_func, name, *args, **kwargs)\n\n    return wrapper\n\n\ndef run_and_report(test_func: Callable, name: str, *test_func_args, **test_func_kwargs):\n    start = time.time()\n    out = test_func(*test_func_args, **test_func_kwargs)\n    elapsed = time.time() - start\n    print(f\"{name} passed in {elapsed:.2f}s.\")\n    if not TEST_FN_PASSED.get(name):\n        report_success(name)\n        TEST_FN_PASSED[name] = True\n    return out\n\n\n@report\ndef test_layernorm_mean_1d(LayerNorm):\n    \"\"\"If an integer is passed, this means normalize over the last dimension which should have that size.\"\"\"\n    x = t.randn(20, 10)\n    ln1 = LayerNorm(10)\n    out = ln1(x)\n    max_mean = out.mean(-1).abs().max().item()\n    assert max_mean < 1e-5, f\"Normalized mean should be about 0, got {max_mean}\"\n\n\n@report\ndef test_layernorm_mean_2d(LayerNorm):\n    \"\"\"If normalized_shape is 2D, should normalize over both the last two dimensions.\"\"\"\n    x = t.randn(20, 10)\n    ln1 = LayerNorm((20, 10))\n    out = ln1(x)\n    max_mean = out.mean((-1, -2)).abs().max().item()\n    assert max_mean < 1e-5, f\"Normalized mean should be about 0, got {max_mean}\"\n\n\n@report\ndef test_layernorm_std(LayerNorm):\n    \"\"\"If epsilon is small enough and no elementwise_affine, the output variance should be very close to 1.\"\"\"\n    x = t.randn(20, 10)\n    ln1 = LayerNorm(10, eps=1e-11, elementwise_affine=False)\n    out = ln1(x)\n    var_diff = (1 - out.var(-1, unbiased=False)).abs().max().item()\n    assert var_diff < 1e-6, f\"Var should be about 1, off by {var_diff}\"\n\n\n@report\ndef test_layernorm_exact(LayerNorm):\n    \"\"\"Your LayerNorm's output should match PyTorch for equal epsilon, up to floating point rounding error.\n\n    This test uses float64 and the result should be extremely tight.\n    \"\"\"\n    x = t.randn(2, 3, 4, 5, dtype=t.float64)\n    # Use large epsilon to make sure it fails if they forget it\n    ln1 = LayerNorm((5,), dtype=t.float64, eps=1e-2)\n    ln2 = t.nn.LayerNorm((5,), dtype=t.float64, eps=1e-2)  # type: ignore\n    actual = ln1(x)\n    expected = ln2(x)\n    allclose(actual, expected)\n\n\n@report\ndef test_layernorm_backward(LayerNorm):\n    \"\"\"The backwards pass should also match PyTorch exactly.\"\"\"\n    x = t.randn(10, 3)\n    x2 = x.clone()\n    x.requires_grad_(True)\n    x2.requires_grad_(True)\n\n    # Without parameters, should be deterministic\n    ref = nn.LayerNorm(3, elementwise_affine=False)\n    ref.requires_grad_(True)\n    ref(x).sum().backward()\n\n    ln = LayerNorm(3, elementwise_affine=False)\n    ln.requires_grad_(True)\n    ln(x2).sum().backward()\n    # Use atol since grad entries are supposed to be zero here\n    assert isinstance(x.grad, t.Tensor)\n    assert isinstance(x2.grad, t.Tensor)\n    allclose_atol(x.grad, x2.grad, atol=1e-5)\n    \n@report\ndef test_embedding(Embedding):\n    \"\"\"Indexing into the embedding should fetch the corresponding rows of the embedding.\"\"\"\n    emb = Embedding(6, 100)\n    out = emb(t.tensor([1, 3, 5], dtype=t.int64))\n    allclose(out[0], emb.weight[1])\n    allclose(out[1], emb.weight[3])\n    allclose(out[2], emb.weight[5])\n\n\n@report\ndef test_embedding_std(Embedding):\n    \"\"\"The standard deviation should be roughly 0.02.\"\"\"\n    t.manual_seed(5)\n    emb = Embedding(6, 100)\n    allclose_scalar_atol(emb.weight.std().item(), 0.02, atol=0.001)\n    \nT = TypeVar(\"T\")\nclass StaticModuleList(nn.ModuleList, Generic[T]):\n    \"\"\"ModuleList where the user vouches that it only contains objects of type T.\n\n    This allows the static checker to work instead of only knowing that the contents are Modules.\n    \"\"\"\n\n    # TBD lowpri: is it possible to do this just with signatures, without actually overriding the method bodies to add a cast?\n\n    def __getitem__(self, index: int) -> T:\n        return cast(T, super().__getitem__(index))\n\n    def __iter__(self) -> Iterator[T]:\n        return cast(Iterator[T], iter(self._modules.values()))\n\n    def __repr__(self):\n        # CM: modified from t.nn.Module.__repr__\n        # We treat the extra repr like the sub-module, one item per line\n        extra_lines = []\n        extra_repr = self.extra_repr()\n        # empty string will be split into list ['']\n        if extra_repr:\n            extra_lines = extra_repr.split(\"\\n\")\n        child_lines = []\n        modules = iter(self._modules.items())\n        key, module = next(modules)\n        n_rest = sum(1 for _ in modules)\n        mod_str = repr(module)\n        mod_str = _addindent(mod_str, 2)\n        child_lines.append(\"(\" + key + \"): \" + mod_str)\n        lines = extra_lines + child_lines + [f\"+ {n_rest} more...\"]\n\n        main_str = self._get_name() + \"(\"\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            if len(extra_lines) == 1 and not child_lines:\n                main_str += extra_lines[0]\n            else:\n                main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n    \n\n    \n\nmem = joblib.Memory(tempfile.gettempdir() + \"/joblib_cache\")\n@mem.cache\ndef load_pretrained_bert() -> BertForMaskedLM:\n    \"\"\"Load the HuggingFace BERT.\n\n    Supresses the spurious warning about some weights not being used.\n    \"\"\"\n    logger = logging.getLogger(\"transformers.modeling_utils\")\n    was_disabled = logger.disabled\n    logger.disabled = True\n    bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n    logger.disabled = was_disabled\n    return cast(BertForMaskedLM, bert)\n\n@report\ndef test_bert_prediction(predict, model, tokenizer):\n    \"\"\"Your Bert should know some names of American presidents.\"\"\"\n    text = \"Former President of the United States of America, George[MASK][MASK]\"\n    predictions = predict(model, tokenizer, text)\n    print(f\"Prompt: {text}\")\n    print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n    assert \"Washington\" in predictions[0]\n    assert \"Bush\" in predictions[0]\n    \ndef remove_hooks(module: t.nn.Module):\n    \"\"\"Remove all hooks from module.\n\n    Use module.apply(remove_hooks) to do this recursively.\n    \"\"\"\n    module._backward_hooks.clear()\n    module._forward_hooks.clear()\n    module._forward_pre_hooks.clear()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:35:46.864097Z","iopub.execute_input":"2023-09-24T03:35:46.864725Z","iopub.status.idle":"2023-09-24T03:35:46.924894Z","shell.execute_reply.started":"2023-09-24T03:35:46.864683Z","shell.execute_reply":"2023-09-24T03:35:46.923652Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"@dataclass(frozen = True)\nclass BertConfig:\n    \n    \"\"\"Constants used throughout the Bert model. Most are self-explanatory.\n\n    intermediate_size is the number of hidden neurons in the MLP (see schematic)\n    type_vocab_size is only used for pretraining on \"next sentence prediction\", which we aren't doing.\n\n    Note that the head size happens to be hidden_size // num_heads, but this isn't necessarily true and your code shouldn't assume it.\n    \"\"\"\n    \n    vocab_size: int = 28996\n    intermediate_size: int = 3072\n    hidden_size: int = 768\n    num_layers: int = 12\n    num_heads: int = 12\n    head_size: int = 64\n    max_position_embeddings: int = 512\n    dropout: float = 0.1\n    type_vocab_size: int = 2\n    layer_norm_epsilon: float = 1e-12\n        \nconfig = BertConfig()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:20:15.882884Z","iopub.execute_input":"2023-09-24T03:20:15.883318Z","iopub.status.idle":"2023-09-24T03:20:15.892000Z","shell.execute_reply.started":"2023-09-24T03:20:15.883283Z","shell.execute_reply":"2023-09-24T03:20:15.890901Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class BertSelfAttention(nn.Module):\n    project_query: nn.Linear\n    project_key: nn.Linear\n    project_value: nn.Linear\n    project_output: nn.Linear\n        \n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.config = config\n        self.project_query = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n        self.project_key = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n        self.project_value = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n        self.project_output = nn.Linear(config.num_heads * config.head_size, config.hidden_size)\n    \n    def attention_pattern_pre_softmax(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq, hidden_size)\n        Return the attention pattern after scaling but before softmax.\n\n        pattern[batch, head, q, k] should be the match between a query at sequence position q and a key at sequence position k.\n        \"\"\"\n        b, s, h = x.shape\n        q = self.project_query(x)\n        k = self.project_key(x)\n        q = rearrange(q, 'b seq (head head_size) -> b head seq head_size', head = self.config.num_heads)\n        k = rearrange(k, 'b seq (head head_size) -> b head seq head_size', head = self.config.num_heads)\n        out = einsum('b head seq_q head_size, b head seq_k head_size -> b head seq_q seq_k', q, k)\n        out = out / self.config.head_size ** 0.5\n        return out\n    \n    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n        \"\"\"\n        additive_attention_mask: shape (batch, head=1, seq_q=1, seq_k) - used in training to prevent copying data from padding tokens. Contains 0 for a real input token and a large negative number for a padding token. If provided, add this to the attention pattern (pre softmax).\n\n        Return: (batch, seq, hidden_size)\n        \"\"\"\n        b,s,h = x.shape\n        attention_pattern = self.attention_pattern_pre_softmax(x)\n        if additive_attention_mask is not None:\n            attention_pattern = attention_pattern + additive_attention_mask\n            \n        softmax_attention = attention_pattern.softmax(dim=-1)\n        v = self.project_value(x)\n        v = rearrange(v, 'b seq (head head_size) -> b head seq head_size', head = self.config.num_heads)\n        weighted_attention = einsum('b head seq_q seq_k, b head seq_k head_size -> b head seq_q head_size',\n                                   softmax_attention, v)\n        weighted_attention = rearrange(weighted_attention, 'b head seq_q head_size -> b seq_q (head head_size)')\n        output = self.project_output(weighted_attention)\n        return output\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:04.900898Z","iopub.execute_input":"2023-09-24T03:32:04.901323Z","iopub.status.idle":"2023-09-24T03:32:04.916262Z","shell.execute_reply.started":"2023-09-24T03:32:04.901288Z","shell.execute_reply":"2023-09-24T03:32:04.914879Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    weight: nn.Parameter\n    bias: nn.Parameter\n        \n    def __init__(\n        self, normalized_shape: Union[int, tuple, t.Size],\n        eps = 1e-05, elementwise_affine = True, device = None,\n        dtype = None,\n    ):\n        super().__init__()\n        if isinstance(normalized_shape, int):\n            normalized_shape = (normalized_shape,)\n            \n        self.normalized_shape = tuple(normalized_shape)\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n        self.normalized_dims = tuple(range(-1, -1 - len(self.normalized_shape), -1))\n        if elementwise_affine:\n            self.weight = nn.Parameter(t.empty(self.normalized_shape, device = device, dtype = dtype))\n            self.bias = nn.Parameter(t.empty(self.normalized_shape, device = device, dtype = dtype))\n        else:\n            self.register_parameter(\"weight\", None)\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n    \n    def reset_parameters(self) -> None:\n        \"\"\"Initialize the weight and bias, if applicable.\"\"\"\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n    \n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"x and the output should both have shape (batch, *).\"\"\"\n        mean = x.mean(dim = self.normalized_dims, keepdim = True)\n        var = x.var(dim = self.normalized_dims, keepdim = True, unbiased = False)\n        output = (x - mean) / ((var + self.eps) ** 0.5)\n        if self.elementwise_affine:\n            output = output * self.weight + self.bias\n        return output\n    \ntest_layernorm_mean_1d(LayerNorm)\ntest_layernorm_mean_2d(LayerNorm)\ntest_layernorm_std(LayerNorm)\ntest_layernorm_exact(LayerNorm)\ntest_layernorm_backward(LayerNorm)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:05.345701Z","iopub.execute_input":"2023-09-24T03:32:05.346110Z","iopub.status.idle":"2023-09-24T03:32:05.366347Z","shell.execute_reply.started":"2023-09-24T03:32:05.346075Z","shell.execute_reply":"2023-09-24T03:32:05.365026Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"__main__.test_layernorm_mean_1d passed in 0.00s.\n__main__.test_layernorm_mean_2d passed in 0.00s.\n__main__.test_layernorm_std passed in 0.00s.\n__main__.test_layernorm_exact passed in 0.00s.\n__main__.test_layernorm_backward passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"class Embedding(nn.Module):\n    num_embeddings: int\n    embedding_dim: int\n    weight: nn.Parameter\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        super().__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.weight = nn.Parameter(t.empty(self.num_embeddings, self.embedding_dim))\n\n    def forward(self, x: t.LongTensor) -> t.Tensor:\n        \"\"\"For each integer in the input, return that row of the embedding.\n\n        Don't convert x to one-hot vectors - this works but is too slow.\n        \"\"\"\n        return self.weight[x]\n\n    def extra_repr(self) -> str:\n        return f\"{self.num_embeddings}, {self.embedding_dim}\"\n    \nassert repr(Embedding(10, 20)) == repr(t.nn.Embedding(10, 20))\ntest_embedding(Embedding)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:05.646349Z","iopub.execute_input":"2023-09-24T03:32:05.647419Z","iopub.status.idle":"2023-09-24T03:32:05.657226Z","shell.execute_reply.started":"2023-09-24T03:32:05.647381Z","shell.execute_reply":"2023-09-24T03:32:05.656068Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"__main__.test_embedding passed in 0.00s.\n","output_type":"stream"}]},{"cell_type":"code","source":"class BertMLP(nn.Module):\n    first_linear: nn.Linear\n    second_linear: nn.Linear\n    layer_norm: LayerNorm\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.first_linear = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.act = nn.GELU()\n        self.second_linear = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        x1 = self.first_linear(x)\n        x1 = self.act(x1)\n        x1 = self.second_linear(x1)\n        x1 = self.dropout(x1)\n        x = self.layer_norm(x1 + x)\n        return x\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:06.019374Z","iopub.execute_input":"2023-09-24T03:32:06.020636Z","iopub.status.idle":"2023-09-24T03:32:06.029651Z","shell.execute_reply.started":"2023-09-24T03:32:06.020595Z","shell.execute_reply":"2023-09-24T03:32:06.028421Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class BertAttention(nn.Module):\n    self_attn: BertSelfAttention\n    layer_norm: LayerNorm\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n        self.self_attn = BertSelfAttention(config)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n        skip = x\n        x = self.self_attn(x, additive_attention_mask)\n        x = self.dropout(x)\n        x = self.layer_norm(x + skip)\n        return x\n    \nclass BertBlock(nn.Module):\n    attention: BertAttention\n    mlp: BertMLP\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.mlp = BertMLP(config)\n        self.attention = BertAttention(config)\n\n    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n        return self.mlp(self.attention(x, additive_attention_mask))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:06.797168Z","iopub.execute_input":"2023-09-24T03:32:06.797579Z","iopub.status.idle":"2023-09-24T03:32:06.808480Z","shell.execute_reply.started":"2023-09-24T03:32:06.797546Z","shell.execute_reply":"2023-09-24T03:32:06.807214Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n    \"\"\"\n    one_zero_attention_mask: shape (batch, seq). Contains 1 if this is a valid token and 0 if it is a padding token.\n    big_negative_number: Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n\n    Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.\n    \"\"\"\n    return rearrange((1 - one_zero_attention_mask) * big_negative_number, \"b k -> b 1 1 k\")\n\nclass BertCommon(nn.Module):\n    token_embedding: Embedding\n    pos_embedding: Embedding\n    token_type_embedding: Embedding\n    layer_norm: LayerNorm\n    blocks: StaticModuleList[BertBlock]\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.token_embedding = Embedding(config.vocab_size, config.hidden_size)\n        self.pos_embedding = Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embedding = Embedding(config.type_vocab_size, config.hidden_size)\n        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks = StaticModuleList([BertBlock(config) for _ in range(config.num_layers)])\n\n    def forward(\n        self,\n        input_ids: t.Tensor,\n        token_type_ids: Optional[t.Tensor] = None,\n        one_zero_attention_mask: Optional[t.Tensor] = None,\n    ) -> t.Tensor:\n        \"\"\"\n        input_ids: (batch, seq) - the token ids\n        token_type_ids: (batch, seq) - only used for next sentence prediction.\n        one_zero_attention_mask: (batch, seq) - only used in training. See make_additive_attention_mask.\n        \"\"\"\n        \n        if token_type_ids is None:\n            token_type_ids = t.zeros_like(input_ids, dtype = t.int64)\n            \n        position = t.arange(input_ids.shape[1]).to(input_ids.device)\n        position = repeat(position, 'n -> b n', b = input_ids.shape[0])\n        \n        if one_zero_attention_mask is None:\n            additive_attention_mask = None\n        else:\n            additive_attention_mask = make_additive_attention_mask(one_zero_attention_mask)\n            \n        x = self.token_embedding(input_ids)\n        x = x + self.pos_embedding(position)\n        x = x + self.token_type_embedding(token_type_ids)\n        x = self.dropout(self.layer_norm(x))\n        for block in self.blocks:\n            x = block(x, additive_attention_mask = additive_attention_mask)\n            \n        return x\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:07.234628Z","iopub.execute_input":"2023-09-24T03:32:07.235356Z","iopub.status.idle":"2023-09-24T03:32:07.249673Z","shell.execute_reply.started":"2023-09-24T03:32:07.235320Z","shell.execute_reply":"2023-09-24T03:32:07.248588Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class BertLanguageModel(nn.Module):\n    common: BertCommon\n    lm_linear: nn.Linear\n    lm_layer_norm: LayerNorm\n    unembed_bias: nn.Parameter\n\n    def __init__(self, config: BertConfig):\n        super().__init__()\n        self.common = BertCommon(config)\n        self.lm_linear = nn.Linear(config.hidden_size, config.hidden_size)\n        self.lm_layer_norm = LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n        self.unembed_bias = nn.Parameter(t.zeros(config.vocab_size))\n\n    def forward(\n        self,\n        input_ids: t.Tensor,\n        token_type_ids: Optional[t.Tensor] = None,\n        one_zero_attention_mask: Optional[t.Tensor] = None,\n    ) -> t.Tensor:\n        \"\"\"Compute logits for each token in the vocabulary.\n\n        Return: shape (batch, seq, vocab_size)\n        \"\"\"\n        x = self.common(input_ids, token_type_ids, one_zero_attention_mask)\n        x = self.lm_linear(x)\n        x = F.gelu(x)\n        x = self.lm_layer_norm(x)\n        x = t.einsum(\"vh,bsh->bsv\", self.common.token_embedding.weight, x)\n        x = x + self.unembed_bias\n        return x\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:07.689948Z","iopub.execute_input":"2023-09-24T03:32:07.690693Z","iopub.status.idle":"2023-09-24T03:32:07.700841Z","shell.execute_reply.started":"2023-09-24T03:32:07.690652Z","shell.execute_reply":"2023-09-24T03:32:07.699764Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def load_pretrained_weights(config: BertConfig) -> BertLanguageModel:\n    hf_bert = load_pretrained_bert()\n\n    def _copy(mine, theirs):\n        mine.detach().copy_(theirs)\n\n    def _copy_weight_bias(mine, theirs, transpose=False):\n        _copy(mine.weight, theirs.weight.T if transpose else theirs.weight)\n        if getattr(mine, \"bias\", None) is not None:\n            _copy(mine.bias, theirs.bias)\n\n    mine = BertLanguageModel(config)\n    # Let's set everything to NaN and then we'll know if we missed one.\n    for name, p in mine.named_parameters():\n        p.requires_grad = False\n        p.fill_(t.nan)\n\n    _copy_weight_bias(mine.common.token_embedding, hf_bert.bert.embeddings.word_embeddings)\n    _copy_weight_bias(mine.common.pos_embedding, hf_bert.bert.embeddings.position_embeddings)\n    _copy_weight_bias(mine.common.token_type_embedding, hf_bert.bert.embeddings.token_type_embeddings)\n    _copy_weight_bias(mine.common.layer_norm, hf_bert.bert.embeddings.LayerNorm)\n\n    # Set up type hints so our autocomplete works properly\n    from transformers.models.bert.modeling_bert import BertLayer\n\n    my_block: BertBlock\n    hf_block: BertLayer\n\n    for my_block, hf_block in zip(mine.common.blocks, hf_bert.bert.encoder.layer):  # type: ignore\n        _copy_weight_bias(my_block.attention.self_attn.project_query, hf_block.attention.self.query)\n        _copy_weight_bias(my_block.attention.self_attn.project_key, hf_block.attention.self.key)\n        _copy_weight_bias(my_block.attention.self_attn.project_value, hf_block.attention.self.value)\n        _copy_weight_bias(my_block.attention.self_attn.project_output, hf_block.attention.output.dense)\n        _copy_weight_bias(my_block.attention.layer_norm, hf_block.attention.output.LayerNorm)\n\n        _copy_weight_bias(my_block.mlp.first_linear, hf_block.intermediate.dense)\n        _copy_weight_bias(my_block.mlp.second_linear, hf_block.output.dense)\n        _copy_weight_bias(my_block.mlp.layer_norm, hf_block.output.LayerNorm)\n\n    _copy_weight_bias(mine.lm_linear, hf_bert.cls.predictions.transform.dense)\n    _copy_weight_bias(mine.lm_layer_norm, hf_bert.cls.predictions.transform.LayerNorm)\n\n    assert t.allclose(\n        hf_bert.bert.embeddings.word_embeddings.weight,\n        hf_bert.cls.predictions.decoder.weight,\n    ), \"Embed and unembed weight should be the same\"\n    # \"Cannot assign non-leaf Tensor to parameter 'weight'\"\n    # mine.unembed.weight = mine.token_embedding.weight\n\n    # Won't remain tied\n    # mine.unembed.weight = hf_bert.bert.embeddings.word_embeddings.weight\n\n    # Won't remain tied during training\n    # mine.unembed.weight.copy_(mine.token_embedding.weight)\n    # mine.unembed.bias.copy_(hf_bert.cls.predictions.decoder.bias)\n\n    # I think works but maybe less good if others have ref to the old Parameter?\n    # mine.unembed_bias = nn.Parameter(input_embeddings.weight.clone())\n\n    mine.unembed_bias.detach().copy_(hf_bert.cls.predictions.decoder.bias)\n\n    fail = False\n    for name, p in mine.named_parameters():\n        if t.isnan(p).any():\n            print(f\"Forgot to initialize: {name}\")\n            fail = True\n        else:\n            p.requires_grad_(True)\n    assert not fail\n    return mine\n    \n\n\n\nmy_bert = load_pretrained_weights(config)\nfor (name, p) in my_bert.named_parameters():\n    assert (\n        p.is_leaf\n    ), \"Parameter {name} is not a leaf node, which will cause problems in training. Try adding detach() somewhere.\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:08.159319Z","iopub.execute_input":"2023-09-24T03:32:08.159704Z","iopub.status.idle":"2023-09-24T03:32:10.246448Z","shell.execute_reply.started":"2023-09-24T03:32:08.159673Z","shell.execute_reply":"2023-09-24T03:32:10.245546Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def predict(model: BertLanguageModel, tokenizer, text: str, k=15) -> List[List[str]]:\n    \"\"\"\n    Return a list of k strings for each [MASK] in the input.\n    \"\"\"\n    model.eval()\n    input_ids = tokenizer(text, return_tensors = 'pt')['input_ids']\n    out = model(input_ids)\n    pred = out[input_ids == tokenizer.mask_token_id]\n    num_masks, vocab = pred.shape\n    tops = pred.topk(k, dim=-1).indices\n    return [[tokenizer.decode(t) for t in mask] for mask in tops]\n    \n\ntokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\ntest_bert_prediction(predict, my_bert, tokenizer)\nyour_text = \"The Answer to the Ultimate Question of Life, The Universe, and Everything is [MASK].\"\npredictions = predict(my_bert, tokenizer, your_text)\nprint(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:32:23.881619Z","iopub.execute_input":"2023-09-24T03:32:23.882042Z","iopub.status.idle":"2023-09-24T03:32:24.155640Z","shell.execute_reply.started":"2023-09-24T03:32:23.882007Z","shell.execute_reply":"2023-09-24T03:32:24.154457Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Prompt: Former President of the United States of America, George[MASK][MASK]\nModel predicted: \n ['W', 'Washington', 'Bush', 'Wallace', 'Dewey', 'Polk', 'Patton', 'H', 'Marshall', 'C', 'Buchanan', 'Clinton', 'G', 'E', 'Carter']\n['.', ';', '?', '!', '|', '...', 'Johnson', ',', 'Press', 'Brown', 'Smith', 'Anderson', 'Carter', 'Jones', 'III']\n__main__.test_bert_prediction passed in 0.09s.\nModel predicted: \n ['Everything', 'Life', 'One', 'Love', 'Good', 'God', 'Nothing', 'Time', 'Here', 'Infinite', 'Space', 'Impossible', 'Free', 'Earth', 'Truth']\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = tokenizer('Hello there', return_tensors='pt')['input_ids']\nexpected = []\n\ndef hook(module, inputs, output):\n    x = inputs[0]\n    out = output[0]\n    expected.append((x, out))\n    \nhf_bert = load_pretrained_bert()\nhf_bert.apply(remove_hooks)\nhf_bert.eval()\nfor layer in hf_bert.bert.encoder.layer:\n    layer.attention.register_forward_hook(hook)\n    layer.register_forward_hook(hook)\nhf_bert(input_ids)\nactual = []\n\ndef my_hook(module, inputs, output):\n    x = inputs[0]\n    actual.append((x, output))\n    \nmy_bert.eval()\nmy_bert.apply(remove_hooks)\nfor layer in my_bert.common.blocks:\n    layer.attention.register_forward_hook(my_hook)\n    layer.register_forward_hook(my_hook)\n    \nmy_bert(input_ids)\n\nassert len(expected) == len(actual)\nfor i, ((ex_in, ex_out), (ac_in, ac_out)) in enumerate(zip(expected, actual)):\n    print(f\"Step {i} input:\", end=\"\")\n    allclose_atol(ac_in, ex_in, atol=1e-5)\n    print('OK')\n    print(f\"Step {i} output:\", end=\"\")\n    allclose_atol(ac_out, ex_out, atol=1e-5)\n    print('OK')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T03:48:12.329011Z","iopub.execute_input":"2023-09-24T03:48:12.329408Z","iopub.status.idle":"2023-09-24T03:48:12.837511Z","shell.execute_reply.started":"2023-09-24T03:48:12.329378Z","shell.execute_reply":"2023-09-24T03:48:12.836369Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Step 0 input:OK\nStep 0 output:OK\nStep 1 input:OK\nStep 1 output:OK\nStep 2 input:OK\nStep 2 output:OK\nStep 3 input:OK\nStep 3 output:OK\nStep 4 input:OK\nStep 4 output:OK\nStep 5 input:OK\nStep 5 output:OK\nStep 6 input:OK\nStep 6 output:OK\nStep 7 input:OK\nStep 7 output:OK\nStep 8 input:OK\nStep 8 output:OK\nStep 9 input:OK\nStep 9 output:OK\nStep 10 input:OK\nStep 10 output:OK\nStep 11 input:OK\nStep 11 output:OK\nStep 12 input:OK\nStep 12 output:OK\nStep 13 input:OK\nStep 13 output:OK\nStep 14 input:OK\nStep 14 output:OK\nStep 15 input:OK\nStep 15 output:OK\nStep 16 input:OK\nStep 16 output:OK\nStep 17 input:OK\nStep 17 output:OK\nStep 18 input:OK\nStep 18 output:OK\nStep 19 input:OK\nStep 19 output:OK\nStep 20 input:OK\nStep 20 output:OK\nStep 21 input:OK\nStep 21 output:OK\nStep 22 input:OK\nStep 22 output:OK\nStep 23 input:OK\nStep 23 output:OK\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}